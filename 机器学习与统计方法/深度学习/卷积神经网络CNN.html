<!DOCTYPE html>
<html lang="en" dir="ltr"><head><title>卷积神经网络CNN</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;family=IBM Plex Mono:wght@400;600&amp;display=swap"/><link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin="anonymous"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="YGC"/><meta property="og:title" content="卷积神经网络CNN"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="卷积神经网络CNN"/><meta name="twitter:description" content="数字识别用传统的机器学习就可以做到蛮高的准确率，使用神经网络，完全也可以用全连接的网络来识别，无非是一个多分类的问题。和我们平时做多分类没有差别。这里我们用卷积神经网络（CNN）来做，CNN在图像识别上应用非常广泛。因为全连接神经网络的输入是一个向量，把图片按照行连接成一个向量，就会把空间信息给抹掉。这就是CNN要解决的，要利用空间信息，来找patterns，比如要识别一只鸟，图片中有一小块是一个鸟嘴，这对识别鸟是有用的。 CNN概述 CNN和全连接的神经网络的区别首先在在于它传递的数据是有形状的，也就是前面说的要利用空间信息，所以用sliding windows的策略，扫描一个又一个的窗口（..."/><meta property="og:description" content="数字识别用传统的机器学习就可以做到蛮高的准确率，使用神经网络，完全也可以用全连接的网络来识别，无非是一个多分类的问题。和我们平时做多分类没有差别。这里我们用卷积神经网络（CNN）来做，CNN在图像识别上应用非常广泛。因为全连接神经网络的输入是一个向量，把图片按照行连接成一个向量，就会把空间信息给抹掉。这就是CNN要解决的，要利用空间信息，来找patterns，比如要识别一只鸟，图片中有一小块是一个鸟嘴，这对识别鸟是有用的。 CNN概述 CNN和全连接的神经网络的区别首先在在于它传递的数据是有形状的，也就是前面说的要利用空间信息，所以用sliding windows的策略，扫描一个又一个的窗口（..."/><meta property="og:image:alt" content="数字识别用传统的机器学习就可以做到蛮高的准确率，使用神经网络，完全也可以用全连接的网络来识别，无非是一个多分类的问题。和我们平时做多分类没有差别。这里我们用卷积神经网络（CNN）来做，CNN在图像识别上应用非常广泛。因为全连接神经网络的输入是一个向量，把图片按照行连接成一个向量，就会把空间信息给抹掉。这就是CNN要解决的，要利用空间信息，来找patterns，比如要识别一只鸟，图片中有一小块是一个鸟嘴，这对识别鸟是有用的。 CNN概述 CNN和全连接的神经网络的区别首先在在于它传递的数据是有形状的，也就是前面说的要利用空间信息，所以用sliding windows的策略，扫描一个又一个的窗口（..."/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https://quartz.jzhao.xyz/机器学习与统计方法/深度学习/卷积神经网络CNN"/><meta property="twitter:url" content="https://quartz.jzhao.xyz/机器学习与统计方法/深度学习/卷积神经网络CNN"/><link rel="icon" href="../../static/icon.png"/><meta name="description" content="数字识别用传统的机器学习就可以做到蛮高的准确率，使用神经网络，完全也可以用全连接的网络来识别，无非是一个多分类的问题。和我们平时做多分类没有差别。这里我们用卷积神经网络（CNN）来做，CNN在图像识别上应用非常广泛。因为全连接神经网络的输入是一个向量，把图片按照行连接成一个向量，就会把空间信息给抹掉。这就是CNN要解决的，要利用空间信息，来找patterns，比如要识别一只鸟，图片中有一小块是一个鸟嘴，这对识别鸟是有用的。 CNN概述 CNN和全连接的神经网络的区别首先在在于它传递的数据是有形状的，也就是前面说的要利用空间信息，所以用sliding windows的策略，扫描一个又一个的窗口（..."/><meta name="generator" content="Quartz"/><link href="../../index.css" rel="stylesheet" type="text/css" data-persist="true"/><style>.expand-button {
  position: absolute;
  display: flex;
  float: right;
  padding: 0.4rem;
  margin: 0.3rem;
  right: 0;
  color: var(--gray);
  border-color: var(--dark);
  background-color: var(--light);
  border: 1px solid;
  border-radius: 5px;
  opacity: 0;
  transition: 0.2s;
}
.expand-button > svg {
  fill: var(--light);
  filter: contrast(0.3);
}
.expand-button:hover {
  cursor: pointer;
  border-color: var(--secondary);
}
.expand-button:focus {
  outline: 0;
}

pre:hover > .expand-button {
  opacity: 1;
  transition: 0.2s;
}

#mermaid-container {
  position: fixed;
  contain: layout;
  z-index: 999;
  left: 0;
  top: 0;
  width: 100vw;
  height: 100vh;
  overflow: hidden;
  display: none;
  backdrop-filter: blur(4px);
  background: rgba(0, 0, 0, 0.5);
}
#mermaid-container.active {
  display: inline-block;
}
#mermaid-container > #mermaid-space {
  border: 1px solid var(--lightgray);
  background-color: var(--light);
  border-radius: 5px;
  position: fixed;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  height: 80vh;
  width: 80vw;
  overflow: hidden;
}
#mermaid-container > #mermaid-space > .mermaid-content {
  position: relative;
  transform-origin: 0 0;
  transition: transform 0.1s ease;
  overflow: visible;
  min-height: 200px;
  min-width: 200px;
}
#mermaid-container > #mermaid-space > .mermaid-content pre {
  margin: 0;
  border: none;
}
#mermaid-container > #mermaid-space > .mermaid-content svg {
  max-width: none;
  height: auto;
}
#mermaid-container > #mermaid-space > .mermaid-controls {
  position: absolute;
  bottom: 20px;
  right: 20px;
  display: flex;
  gap: 8px;
  padding: 8px;
  background: var(--light);
  border: 1px solid var(--lightgray);
  border-radius: 6px;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
  z-index: 2;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  padding: 0;
  border: 1px solid var(--lightgray);
  background: var(--light);
  color: var(--dark);
  border-radius: 4px;
  cursor: pointer;
  font-size: 16px;
  font-family: var(--bodyFont);
  transition: all 0.2s ease;
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:hover {
  background: var(--lightgray);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:active {
  transform: translateY(1px);
}
#mermaid-container > #mermaid-space > .mermaid-controls .mermaid-control-button:nth-child(2) {
  width: auto;
  padding: 0 12px;
  font-size: 14px;
}
/*# sourceMappingURL=data:application/json;charset=utf-8;base64,eyJ2ZXJzaW9uIjozLCJzb3VyY2VSb290IjoiL2hvbWUvcnVubmVyL3dvcmsveWdjLWJsb2cveWdjLWJsb2cvYmxvZy9xdWFydHovY29tcG9uZW50cy9zdHlsZXMiLCJzb3VyY2VzIjpbIm1lcm1haWQuaW5saW5lLnNjc3MiXSwibmFtZXMiOltdLCJtYXBwaW5ncyI6IkFBQUE7RUFDRTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTs7QUFFQTtFQUNFO0VBQ0E7O0FBR0Y7RUFDRTtFQUNBOztBQUdGO0VBQ0U7OztBQUtGO0VBQ0U7RUFDQTs7O0FBSUo7RUFDRTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBOztBQUVBO0VBQ0U7O0FBR0Y7RUFDRTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTs7QUFFQTtFQUNFO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTs7QUFFQTtFQUNFO0VBQ0E7O0FBR0Y7RUFDRTtFQUNBOztBQUlKO0VBQ0U7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTs7QUFFQTtFQUNFO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7RUFDQTtFQUNBO0VBQ0E7O0FBRUE7RUFDRTs7QUFHRjtFQUNFOztBQUlGO0VBQ0U7RUFDQTtFQUNBIiwic291cmNlc0NvbnRlbnQiOlsiLmV4cGFuZC1idXR0b24ge1xuICBwb3NpdGlvbjogYWJzb2x1dGU7XG4gIGRpc3BsYXk6IGZsZXg7XG4gIGZsb2F0OiByaWdodDtcbiAgcGFkZGluZzogMC40cmVtO1xuICBtYXJnaW46IDAuM3JlbTtcbiAgcmlnaHQ6IDA7IC8vIE5PVEU6IHJpZ2h0IHdpbGwgYmUgc2V0IGluIG1lcm1haWQuaW5saW5lLnRzXG4gIGNvbG9yOiB2YXIoLS1ncmF5KTtcbiAgYm9yZGVyLWNvbG9yOiB2YXIoLS1kYXJrKTtcbiAgYmFja2dyb3VuZC1jb2xvcjogdmFyKC0tbGlnaHQpO1xuICBib3JkZXI6IDFweCBzb2xpZDtcbiAgYm9yZGVyLXJhZGl1czogNXB4O1xuICBvcGFjaXR5OiAwO1xuICB0cmFuc2l0aW9uOiAwLjJzO1xuXG4gICYgPiBzdmcge1xuICAgIGZpbGw6IHZhcigtLWxpZ2h0KTtcbiAgICBmaWx0ZXI6IGNvbnRyYXN0KDAuMyk7XG4gIH1cblxuICAmOmhvdmVyIHtcbiAgICBjdXJzb3I6IHBvaW50ZXI7XG4gICAgYm9yZGVyLWNvbG9yOiB2YXIoLS1zZWNvbmRhcnkpO1xuICB9XG5cbiAgJjpmb2N1cyB7XG4gICAgb3V0bGluZTogMDtcbiAgfVxufVxuXG5wcmUge1xuICAmOmhvdmVyID4gLmV4cGFuZC1idXR0b24ge1xuICAgIG9wYWNpdHk6IDE7XG4gICAgdHJhbnNpdGlvbjogMC4ycztcbiAgfVxufVxuXG4jbWVybWFpZC1jb250YWluZXIge1xuICBwb3NpdGlvbjogZml4ZWQ7XG4gIGNvbnRhaW46IGxheW91dDtcbiAgei1pbmRleDogOTk5O1xuICBsZWZ0OiAwO1xuICB0b3A6IDA7XG4gIHdpZHRoOiAxMDB2dztcbiAgaGVpZ2h0OiAxMDB2aDtcbiAgb3ZlcmZsb3c6IGhpZGRlbjtcbiAgZGlzcGxheTogbm9uZTtcbiAgYmFja2Ryb3AtZmlsdGVyOiBibHVyKDRweCk7XG4gIGJhY2tncm91bmQ6IHJnYmEoMCwgMCwgMCwgMC41KTtcblxuICAmLmFjdGl2ZSB7XG4gICAgZGlzcGxheTogaW5saW5lLWJsb2NrO1xuICB9XG5cbiAgJiA+ICNtZXJtYWlkLXNwYWNlIHtcbiAgICBib3JkZXI6IDFweCBzb2xpZCB2YXIoLS1saWdodGdyYXkpO1xuICAgIGJhY2tncm91bmQtY29sb3I6IHZhcigtLWxpZ2h0KTtcbiAgICBib3JkZXItcmFkaXVzOiA1cHg7XG4gICAgcG9zaXRpb246IGZpeGVkO1xuICAgIHRvcDogNTAlO1xuICAgIGxlZnQ6IDUwJTtcbiAgICB0cmFuc2Zvcm06IHRyYW5zbGF0ZSgtNTAlLCAtNTAlKTtcbiAgICBoZWlnaHQ6IDgwdmg7XG4gICAgd2lkdGg6IDgwdnc7XG4gICAgb3ZlcmZsb3c6IGhpZGRlbjtcblxuICAgICYgPiAubWVybWFpZC1jb250ZW50IHtcbiAgICAgIHBvc2l0aW9uOiByZWxhdGl2ZTtcbiAgICAgIHRyYW5zZm9ybS1vcmlnaW46IDAgMDtcbiAgICAgIHRyYW5zaXRpb246IHRyYW5zZm9ybSAwLjFzIGVhc2U7XG4gICAgICBvdmVyZmxvdzogdmlzaWJsZTtcbiAgICAgIG1pbi1oZWlnaHQ6IDIwMHB4O1xuICAgICAgbWluLXdpZHRoOiAyMDBweDtcblxuICAgICAgcHJlIHtcbiAgICAgICAgbWFyZ2luOiAwO1xuICAgICAgICBib3JkZXI6IG5vbmU7XG4gICAgICB9XG5cbiAgICAgIHN2ZyB7XG4gICAgICAgIG1heC13aWR0aDogbm9uZTtcbiAgICAgICAgaGVpZ2h0OiBhdXRvO1xuICAgICAgfVxuICAgIH1cblxuICAgICYgPiAubWVybWFpZC1jb250cm9scyB7XG4gICAgICBwb3NpdGlvbjogYWJzb2x1dGU7XG4gICAgICBib3R0b206IDIwcHg7XG4gICAgICByaWdodDogMjBweDtcbiAgICAgIGRpc3BsYXk6IGZsZXg7XG4gICAgICBnYXA6IDhweDtcbiAgICAgIHBhZGRpbmc6IDhweDtcbiAgICAgIGJhY2tncm91bmQ6IHZhcigtLWxpZ2h0KTtcbiAgICAgIGJvcmRlcjogMXB4IHNvbGlkIHZhcigtLWxpZ2h0Z3JheSk7XG4gICAgICBib3JkZXItcmFkaXVzOiA2cHg7XG4gICAgICBib3gtc2hhZG93OiAwIDJweCA0cHggcmdiYSgwLCAwLCAwLCAwLjEpO1xuICAgICAgei1pbmRleDogMjtcblxuICAgICAgLm1lcm1haWQtY29udHJvbC1idXR0b24ge1xuICAgICAgICBkaXNwbGF5OiBmbGV4O1xuICAgICAgICBhbGlnbi1pdGVtczogY2VudGVyO1xuICAgICAgICBqdXN0aWZ5LWNvbnRlbnQ6IGNlbnRlcjtcbiAgICAgICAgd2lkdGg6IDMycHg7XG4gICAgICAgIGhlaWdodDogMzJweDtcbiAgICAgICAgcGFkZGluZzogMDtcbiAgICAgICAgYm9yZGVyOiAxcHggc29saWQgdmFyKC0tbGlnaHRncmF5KTtcbiAgICAgICAgYmFja2dyb3VuZDogdmFyKC0tbGlnaHQpO1xuICAgICAgICBjb2xvcjogdmFyKC0tZGFyayk7XG4gICAgICAgIGJvcmRlci1yYWRpdXM6IDRweDtcbiAgICAgICAgY3Vyc29yOiBwb2ludGVyO1xuICAgICAgICBmb250LXNpemU6IDE2cHg7XG4gICAgICAgIGZvbnQtZmFtaWx5OiB2YXIoLS1ib2R5Rm9udCk7XG4gICAgICAgIHRyYW5zaXRpb246IGFsbCAwLjJzIGVhc2U7XG5cbiAgICAgICAgJjpob3ZlciB7XG4gICAgICAgICAgYmFja2dyb3VuZDogdmFyKC0tbGlnaHRncmF5KTtcbiAgICAgICAgfVxuXG4gICAgICAgICY6YWN0aXZlIHtcbiAgICAgICAgICB0cmFuc2Zvcm06IHRyYW5zbGF0ZVkoMXB4KTtcbiAgICAgICAgfVxuXG4gICAgICAgIC8vIFN0eWxlIHRoZSByZXNldCBidXR0b24gZGlmZmVyZW50bHlcbiAgICAgICAgJjpudGgtY2hpbGQoMikge1xuICAgICAgICAgIHdpZHRoOiBhdXRvO1xuICAgICAgICAgIHBhZGRpbmc6IDAgMTJweDtcbiAgICAgICAgICBmb250LXNpemU6IDE0cHg7XG4gICAgICAgIH1cbiAgICAgIH1cbiAgICB9XG4gIH1cbn1cbiJdfQ== */</style><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" data-persist="true"/><script src="../../prescript.js" type="application/javascript" data-persist="true"></script><script type="application/javascript" data-persist="true">const fetchData = fetch("../../static/contentIndex.json").then(data => data.json())</script><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="https://quartz.jzhao.xyz/index.xml"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image" content="https://quartz.jzhao.xyz/机器学习与统计方法/深度学习/卷积神经网络CNN-og-image.webp"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/机器学习与统计方法/深度学习/卷积神经网络CNN-og-image.webp"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/机器学习与统计方法/深度学习/卷积神经网络CNN-og-image.webp"/><meta property="og:image:type" content="image/.webp"/></head><body data-slug="机器学习与统计方法/深度学习/卷积神经网络CNN"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href="../..">YGC</a></h2><div class="spacer mobile-only"></div><div class="flex-component" style="flex-direction: row; flex-wrap: nowrap; gap: 1rem;"><div style="flex-grow: 1; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><div class="search"><button class="search-button"><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg><p>Search</p></button><div class="search-container"><div class="search-space"><input autocomplete="off" class="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div class="search-layout" data-preview="true"></div></div></div></div></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div style="flex-grow: 0; flex-shrink: 1; flex-basis: auto; order: 0; align-self: center; justify-self: center;"><button class="readermode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" class="readerIcon" fill="currentColor" stroke="currentColor" stroke-width="0.2" stroke-linecap="round" stroke-linejoin="round" width="64px" height="64px" viewBox="0 0 24 24" aria-label="Reader mode"><title>Reader mode</title><g transform="translate(-1.8, -1.8) scale(1.15, 1.2)"><path d="M8.9891247,2.5 C10.1384702,2.5 11.2209868,2.96705384 12.0049645,3.76669482 C12.7883914,2.96705384 13.8709081,2.5 15.0202536,2.5 L18.7549359,2.5 C19.1691495,2.5 19.5049359,2.83578644 19.5049359,3.25 L19.5046891,4.004 L21.2546891,4.00457396 C21.6343849,4.00457396 21.9481801,4.28672784 21.9978425,4.6528034 L22.0046891,4.75457396 L22.0046891,20.25 C22.0046891,20.6296958 21.7225353,20.943491 21.3564597,20.9931534 L21.2546891,21 L2.75468914,21 C2.37499337,21 2.06119817,20.7178461 2.01153575,20.3517706 L2.00468914,20.25 L2.00468914,4.75457396 C2.00468914,4.37487819 2.28684302,4.061083 2.65291858,4.01142057 L2.75468914,4.00457396 L4.50368914,4.004 L4.50444233,3.25 C4.50444233,2.87030423 4.78659621,2.55650904 5.15267177,2.50684662 L5.25444233,2.5 L8.9891247,2.5 Z M4.50368914,5.504 L3.50468914,5.504 L3.50468914,19.5 L10.9478955,19.4998273 C10.4513189,18.9207296 9.73864328,18.5588115 8.96709342,18.5065584 L8.77307039,18.5 L5.25444233,18.5 C4.87474657,18.5 4.56095137,18.2178461 4.51128895,17.8517706 L4.50444233,17.75 L4.50368914,5.504 Z M19.5049359,17.75 C19.5049359,18.1642136 19.1691495,18.5 18.7549359,18.5 L15.2363079,18.5 C14.3910149,18.5 13.5994408,18.8724714 13.0614828,19.4998273 L20.5046891,19.5 L20.5046891,5.504 L19.5046891,5.504 L19.5049359,17.75 Z M18.0059359,3.999 L15.0202536,4 L14.8259077,4.00692283 C13.9889509,4.06666544 13.2254227,4.50975805 12.7549359,5.212 L12.7549359,17.777 L12.7782651,17.7601316 C13.4923805,17.2719483 14.3447024,17 15.2363079,17 L18.0059359,16.999 L18.0056891,4.798 L18.0033792,4.75457396 L18.0056891,4.71 L18.0059359,3.999 Z M8.9891247,4 L6.00368914,3.999 L6.00599909,4.75457396 L6.00599909,4.75457396 L6.00368914,4.783 L6.00368914,16.999 L8.77307039,17 C9.57551536,17 10.3461406,17.2202781 11.0128313,17.6202194 L11.2536891,17.776 L11.2536891,5.211 C10.8200889,4.56369974 10.1361548,4.13636104 9.37521067,4.02745763 L9.18347055,4.00692283 L8.9891247,4 Z"></path></g></svg></button></div></div><div class="explorer" data-behavior="link" data-collapsed="collapsed" data-savestate="true" data-data-fns="{&quot;order&quot;:[&quot;filter&quot;,&quot;map&quot;,&quot;sort&quot;],&quot;sortFn&quot;:&quot;(a,b)=>!a.isFolder&amp;&amp;!b.isFolder||a.isFolder&amp;&amp;b.isFolder?a.displayName.localeCompare(b.displayName,void 0,{numeric:!0,sensitivity:\&quot;base\&quot;}):!a.isFolder&amp;&amp;b.isFolder?1:-1&quot;,&quot;filterFn&quot;:&quot;node=>node.slugSegment!==\&quot;tags\&quot;&quot;,&quot;mapFn&quot;:&quot;node=>node&quot;}"><button type="button" class="explorer-toggle mobile-explorer hide-until-loaded" data-mobile="true" aria-controls="explorer-52"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide-menu"><line x1="4" x2="20" y1="12" y2="12"></line><line x1="4" x2="20" y1="6" y2="6"></line><line x1="4" x2="20" y1="18" y2="18"></line></svg></button><button type="button" class="title-button explorer-toggle desktop-explorer" data-mobile="false" aria-expanded="true"><h2>Explorer</h2><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="explorer-52" class="explorer-content" aria-expanded="false" role="group"><ul class="explorer-ul overflow" id="list-0"><li class="overflow-end"></li></ul></div><template id="template-file"><li><a href="#"></a></li></template><template id="template-folder"><li><div class="folder-container"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="5 8 14 8" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="folder-icon"><polyline points="6 9 12 15 18 9"></polyline></svg><div><button class="folder-button"><span class="folder-title"></span></button></div></div><div class="folder-outer"><ul class="content"></ul></div></li></template></div></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="../../">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../机器学习与统计方法/">机器学习与统计方法</a><p> ❯ </p></div><div class="breadcrumb-element"><a href="../../机器学习与统计方法/深度学习/">深度学习</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>卷积神经网络CNN</a></div></nav><h1 class="article-title">卷积神经网络CNN</h1><p show-comma="true" class="content-meta"><time datetime="2025-09-15T00:00:00.000Z">Sep 15, 2025</time><span>14 min read</span></p><ul class="tags"><li><a href="../../tags/DeepLearning" class="internal tag-link">DeepLearning</a></li><li><a href="../../tags/CNN" class="internal tag-link">CNN</a></li><li><a href="../../tags/Pytorch" class="internal tag-link">Pytorch</a></li><li><a href="../../tags/Classification" class="internal tag-link">Classification</a></li></ul></div></div><article class="popover-hint"><p>数字识别用传统的机器学习就可以做到蛮高的准确率，使用神经网络，完全也可以用全连接的网络来识别，无非是一个<a href="https://mp.weixin.qq.com/s/hnnXhYl_0I0Pr074knkINg" class="external">多分类的问题<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>。和我们平时做多分类没有差别。这里我们用卷积神经网络（CNN）来做，CNN在图像识别上应用非常广泛。因为全连接神经网络的输入是一个向量，把图片按照行连接成一个向量，就会把空间信息给抹掉。这就是CNN要解决的，要利用空间信息，来找patterns，比如要识别一只鸟，图片中有一小块是一个鸟嘴，这对识别鸟是有用的。</p>
<h2 id="cnn概述">CNN概述<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#cnn概述" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>CNN和全连接的神经网络的区别首先在在于它传递的数据是有形状的，也就是前面说的要利用空间信息，所以用sliding windows的策略，扫描一个又一个的窗口（patches），这个窗口每次扫描就用一个滤波器（filter）去做inner product，一个窗口出来是一个值。</p>
<p><img src="./assets/卷积神经网络CNN/file-20251209131527376.png" alt/></p>
<p>这一个滤波器作用于这一个通道上，会出来另一个大小不一样的二维数据，所以输入输出有时候又可以统一称之为特征图。输入通常不是一个通道，比如说RGB的图，就有RGB三个通道，最终的输出是每一个通道都一样的做法，然后将多个通道进行加和。</p>
<p><img src="./assets/卷积神经网络CNN/file-20251209131532178.png" alt/></p>
<p>滤波器主要用于提取图像的特征，比如下面这张图所演示的。</p>
<p><img src="./assets/卷积神经网络CNN/file-20251209131537554.png" alt/></p>
<p>一个滤波器出来一张特征图，多个滤波器就会出来多个特征图，就像RGB的图有三个通道一样，用N个滤波器，出来的数据就有N个通道，每个通道是2D的数据，整个是一个3D的数据，我们依然可以理解成一张图，原来的图是3个通道，现在有N个通道了，不是我们理解的RGB图像，但形式上是一致的。这就是卷积的过程。</p>
<p><img src="./assets/卷积神经网络CNN/file-20251209131542914.png" alt/></p>
<p>滤波器里的数值，相当于我们在全连接神经网络中权重，也会加上偏置，这些参数都是学习出来的。</p>
<p><img src="./assets/卷积神经网络CNN/file-20251209131548297.png" alt/></p>
<p>一个典型的卷积神经网络就是卷积<span>→</span>激活<span>→</span>池化，可以是一个这样的过程，也可以连着搭几个。这个做为Feature learning，把图像的特征学出来，然后用这些特征做为输入，接一个全连接的神经网络（也被称之为dense network），这就是完整的CNN。</p>
<h2 id="卷积层">卷积层<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#卷积层" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p><img src="./assets/卷积神经网络CNN/file-20251209131553531.png" alt/></p>
<p>运算就是前面讲到的相当于图像处理的滤波器运算。这里涉及到一些和运算细则有关的概念，包括kernel size, padding和stride，这些会影响到输出的数据形状大小，这也要算一下的，因为最终要过一下全连接层，输入的大小是要准确指定的。</p>
<h3 id="滤波器filterkernel">滤波器（filter/kernel）<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#滤波器filterkernel" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>滤波器有时候也叫kernel，因为滤波器是不需要我们指定的，也是学出来的，我们要指定的只有滤波器的数量，也是输出的channel数目，还有滤波器的大小，kernel size。</p>
<h3 id="填充padding">填充（padding）<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#填充padding" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>一个4x4的数据，用一个3x3的滤波器，输出的数据就变成了2x2，所以这样子，如果叠加卷积层，就有可能在中间变成1x1，后面没法再继续做卷积了。所以就需要在数据的边缘进行填充，像下面这个例子，通过填充就可以让输入和输出保持4x4的形状。</p>
<p><img src="./assets/卷积神经网络CNN/file-20251209131559098.png" alt/></p>
<h3 id="步幅stride">步幅（stride）<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#步幅stride" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>在应用滤波器时，通过移动窗口，把所有的数据都过一遍，窗口和窗口之间是有重叠的，移动窗口的步伐有多大，就是步幅。</p>
<p><img src="./assets/卷积神经网络CNN/file-20251209131604049.png" alt/></p>
<h2 id="池化">池化<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#池化" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>典型的CNN包括卷积<span>→</span>激活<span>→</span>池化，激活就是<code>ReLU</code>这些，都很清楚，最后就池化：</p>
<p><img src="./assets/卷积神经网络CNN/file-20251209131608989.png" alt/></p>
<p>池化没有需要学习的参数，就是在目标区域里取个代表性的值，上图是最大值，也可以是平均值。池化是在每个通道单独进行，所以不会改变通道数。</p>
<p>输入数据发生一点微小的变化，通过池化，输出基本上是不变的，这样就具有一定的鲁棒性。当然其实池化最主要的作用是让数据变小，这本身是为了更高效的计算。从这个角度来说，计算资源够的情况下，是可以不需要池化的。</p>
<p>书上讲的都是说让数据变小这一点，我自己想到的是，因为卷积运算可以套好几个，那么经过池化之后，再做同样大小的滤波运算，相当于在更大的区域去捕获特征了。我认为是有这个作用的，相当于前面是低空看细节，后面高空看轮廓，有这样一个过程的作用。</p>
<p>另外也要看实际情况，对于一般的图像处理来说，池化的过程，相当于压缩了图片，压缩完图片看着变化不大。AlphaGO就使用了CNN，但是它就没有用池化，因为池化了之后，棋盘就不完整了，这和图像处理是有区别的，所以它不能够用池化。所以说，还得具体情况具体分析。</p>
<h2 id="pytorch实战">PyTorch实战<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#pytorch实战" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<h3 id="数据">数据<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#数据" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>这里使用的是MNIST手写数字的数据集，有6万张图片。先创建文件夹，然后下载，如果文件已经存在，就跳过下载。</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>from pathlib import Path</span></span>
<span data-line><span>import requests</span></span>
<span data-line> </span>
<span data-line><span>DATA_PATH = Path(&quot;data&quot;)</span></span>
<span data-line><span>PATH = DATA_PATH / &quot;mnist&quot;</span></span>
<span data-line> </span>
<span data-line><span>PATH.mkdir(parents=True, exist_ok=True)</span></span>
<span data-line> </span>
<span data-line><span>URL = &quot;https://github.com/pytorch/tutorials/raw/main/_static/&quot;</span></span>
<span data-line><span>FILENAME = &quot;mnist.pkl.gz&quot;</span></span>
<span data-line> </span>
<span data-line><span>if not (PATH / FILENAME).exists():</span></span>
<span data-line><span>    content = requests.get(URL + FILENAME).content</span></span>
<span data-line><span>    (PATH / FILENAME).open(&quot;wb&quot;).write(content)</span></span></code></pre></figure>
<p>这个数据是个<code>pickle</code>文件，存的是numpy array格式，我们相应地把它解压缩，讲进来。</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>import pickle</span></span>
<span data-line><span>import gzip</span></span>
<span data-line> </span>
<span data-line><span>with gzip.open((PATH / FILENAME).as_posix(), &quot;rb&quot;) as f:</span></span>
<span data-line><span>    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=&quot;latin-1&quot;)</span></span></code></pre></figure>
<p>总共有6万张图片，5万张是训练集，1万张做为验证/测试集。</p>
<p><img src="https://qcn657qp2atl.feishu.cn/space/api/box/stream/download/asynccode/?code=ZGQ1Y2EwZjBmYmFlYTE5N2RiMTlkZTM2N2U0YzllNmRfUVJDSjRRN09LQXNiQzVmUkpSdE5BT3JhbEpENTFtdEZfVG9rZW46QnRzMmJEQWQyb1JmM2J4OFhRUGNSNkFSbmZiXzE3NjUyNTczMjA6MTc2NTI2MDkyMF9WNA" alt/></p>
<p>图片是28x28像素的，已经被打成长度为784的一维向量。</p>
<p>我们可以随便选一张画出来看看：</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>from matplotlib import pyplot as plt</span></span>
<span data-line><span>import numpy as np</span></span>
<span data-line><span>plt.imshow(x_train[9].reshape((28,28)), cmap='gray')</span></span></code></pre></figure>
<p><img src="./assets/卷积神经网络CNN/file-20251209131617788.png" alt/></p>
<h4 id="numpy-to-tensor">NumPy to Tensor<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#numpy-to-tensor" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<p>因为是numpy array格式，我们需要转换成tensor。</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>import torch</span></span>
<span data-line> </span>
<span data-line><span>x_train, y_train, x_valid, y_valid = map(</span></span>
<span data-line><span>    torch.tensor, (x_train, y_train, x_valid, y_valid)</span></span>
<span data-line><span>)</span></span></code></pre></figure>
<h4 id="dataloader">DataLoader<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#dataloader" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>from torch.utils.data import TensorDataset</span></span>
<span data-line><span>from torch.utils.data import DataLoader</span></span>
<span data-line> </span>
<span data-line><span>bs = 64</span></span>
<span data-line> </span>
<span data-line><span>train_ds = TensorDataset(x_train, y_train)</span></span>
<span data-line><span>train_dl = DataLoader(train_ds, batch_size=bs)</span></span>
<span data-line> </span>
<span data-line><span>valid_ds = TensorDataset(x_valid, y_valid)</span></span>
<span data-line><span>valid_dl = DataLoader(valid_ds, batch_size=bs * 2)</span></span></code></pre></figure>
<p>我们使用<code>TensorDataSet</code>来存这些数据，同时存samples和labels，再用<code>DataLoader</code>来方便我们访问和迭代这些数据，其中一个参数是batch_size，也就是批处理的大小，指定一次有多少个数据打包喂给神经网络。</p>
<h3 id="模型">模型<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#模型" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h3>
<p>首先是看看有没有GPU，有则设备设为’cuda’，没有就设为’cpu’。我们的运算都在指定的device上进行。</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')</span></span></code></pre></figure>
<h4 id="训练步骤">训练步骤<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#训练步骤" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<p>训练的代码都基本一样，我们把它写成一个函数，方便调用。</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>def train_step(model: torch.nn.Module,</span></span>
<span data-line><span>               data_loader,</span></span>
<span data-line><span>               loss_fn,</span></span>
<span data-line><span>               optimizer,</span></span>
<span data-line><span>               acc_fn,</span></span>
<span data-line><span>               device = device):</span></span>
<span data-line> </span>
<span data-line><span>    train_loss, train_acc = 0, 0</span></span>
<span data-line><span>    for batch, (X, y) in enumerate(data_loader):</span></span>
<span data-line><span>        X, y = X.to(device), y.to(device)</span></span>
<span data-line><span>        model.train()</span></span>
<span data-line><span>        y_pred = model(X)</span></span>
<span data-line> </span>
<span data-line><span>        loss = loss_fn(y_pred, y)</span></span>
<span data-line><span>        train_loss += loss</span></span>
<span data-line><span>        train_acc += acc_fn(y_pred, y)</span></span>
<span data-line><span>        </span></span>
<span data-line><span>        optimizer.zero_grad()</span></span>
<span data-line><span>        loss.backward()</span></span>
<span data-line><span>        optimizer.step()</span></span>
<span data-line> </span>
<span data-line><span>    train_loss /= len(data_loader)</span></span>
<span data-line><span>    train_acc /= len(data_loader)</span></span>
<span data-line> </span>
<span data-line><span>    print(f&quot;Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}&quot;)</span></span></code></pre></figure>
<h4 id="测试步骤">测试步骤<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#测试步骤" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<p>测试步骤也是同样的道理，也写成函数：</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>def test_step(model,</span></span>
<span data-line><span>              data_loader,</span></span>
<span data-line><span>              loss_fn,</span></span>
<span data-line><span>              acc_fn,</span></span>
<span data-line><span>              device = device):</span></span>
<span data-line> </span>
<span data-line><span>    test_loss, test_acc=0, 0</span></span>
<span data-line><span>    model.eval()</span></span>
<span data-line><span>    with torch.inference_mode():</span></span>
<span data-line><span>        for X, y in data_loader:</span></span>
<span data-line><span>            X, y = X.to(device), y.to(device)</span></span>
<span data-line><span>            </span></span>
<span data-line><span>            test_pred = model(X)</span></span>
<span data-line> </span>
<span data-line><span>            test_loss += loss_fn(test_pred, y)</span></span>
<span data-line><span>            test_acc += acc_fn(test_pred.argmax(dim=1), y)</span></span>
<span data-line><span>            </span></span>
<span data-line><span>        test_loss /= len(data_loader)</span></span>
<span data-line><span>        test_acc /= len(data_loader)</span></span>
<span data-line> </span>
<span data-line><span>        print(f&quot;Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}\n&quot;)</span></span></code></pre></figure>
<h4 id="cnn模型">CNN模型<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#cnn模型" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<p>我们用<a href="https://poloclub.github.io/cnn-explainer/%E9%87%8C%E7%9A%84CNN%E6%9E%B6%E6%9E%84%EF%BC%8C%E5%8C%85%E5%90%AB%E4%B8%A4%E4%B8%AA%E7%BB%93%E6%9E%84%E4%B8%80%E6%A0%B7%E7%9A%84blocks%E3%80%82" class="external">https://poloclub.github.io/cnn-explainer/里的CNN架构，包含两个结构一样的blocks。<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
<p><img src="./assets/卷积神经网络CNN/file-20251209131627855.png" alt/></p>
<p>这个架构是有名的VGG的简化版本，TinyVGG:</p>
<p><img src="./assets/卷积神经网络CNN/file-20251209131637262.png" alt/></p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>from torch import nn</span></span>
<span data-line> </span>
<span data-line><span>class MNIST(nn.Module):</span></span>
<span data-line><span>    def __init__(self, input_shape, hidden_units, output_shape):</span></span>
<span data-line><span>        super().__init__()</span></span>
<span data-line><span>        self.block_1 = nn.Sequential(</span></span>
<span data-line><span>            nn.Unflatten(1, (1, 28, 28)),</span></span>
<span data-line><span>            nn.Conv2d(</span></span>
<span data-line><span>                in_channels = input_shape,</span></span>
<span data-line><span>                out_channels = hidden_units,</span></span>
<span data-line><span>                kernel_size = 3,</span></span>
<span data-line><span>                stride=1,</span></span>
<span data-line><span>                padding=1),</span></span>
<span data-line><span>            nn.ReLU(),</span></span>
<span data-line><span>            nn.Conv2d(</span></span>
<span data-line><span>                hidden_units, hidden_units, 3, stride=1, padding=1),</span></span>
<span data-line><span>            nn.ReLU(),</span></span>
<span data-line><span>            nn.MaxPool2d(kernel_size=2,</span></span>
<span data-line><span>                         stride=2)</span></span>
<span data-line><span>        )</span></span>
<span data-line> </span>
<span data-line><span>        self.block_2 = nn.Sequential(</span></span>
<span data-line><span>            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),</span></span>
<span data-line><span>            nn.ReLU(),</span></span>
<span data-line><span>            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),</span></span>
<span data-line><span>            nn.ReLU(),</span></span>
<span data-line><span>            nn.MaxPool2d(kernel_size=2, stride=2)</span></span>
<span data-line><span>        )</span></span>
<span data-line> </span>
<span data-line><span>        self.classifier = nn.Sequential(</span></span>
<span data-line><span>            nn.Flatten(),</span></span>
<span data-line><span>            nn.Linear(in_features=hidden_units*7*7,</span></span>
<span data-line><span>                      out_features=output_shape)</span></span>
<span data-line><span>        )</span></span>
<span data-line> </span>
<span data-line><span>    def forward(self, x):</span></span>
<span data-line><span>        return self.classifier(self.block_2(self.block_1(x)))</span></span></code></pre></figure>
<p>两个blocks都是<code>Conv2d</code><span>→</span><code>ReLU</code><span>→</span><code>Conv2d</code><span>→</span><code>ReLU</code><span>→</span><code>MaxPool2d</code>。参数前面都有解析过相应的概念了，这里需要讲的是在<code>block_1</code>里用了<code>nn.Unflatten</code>，是因为原来的数据28x28像素已经被转成一维向量了，所以此处需要搞回2维的数据。然后这里的卷积运算，用的kernel_size=3，stride=1，padding=1，这样子数据的形状大小是不变的；再通过<code>MaxPool2d</code>的时候，kernel_size=2, stride=2，所以一个2x2的数据就变成了1个数，数据就从28x28，变成了14x14。有两个blocks，过了两次MaxPooling，最终就变成了7x7，所以最后给nn.Linear的数据，就是7x7x<code>hidden_units</code>（也就是最后数据的通道数）。</p>
<h4 id="训练模型">训练模型<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#训练模型" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<p>先初始化一个模型，input_shape就是传给卷积层的通道数，这里是灰度图，所以只有一个通道，hidden_units就是中间的通道数，而output_shape就是最后过全连接神经网络的输出，因为是0-9的数据，所以是10个分类。</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>torch.manual_seed(42)</span></span>
<span data-line> </span>
<span data-line><span>model = MNIST(input_shape=1, hidden_units=10, output_shape=10).to(device)</span></span></code></pre></figure>
<p><img src="https://qcn657qp2atl.feishu.cn/space/api/box/stream/download/asynccode/?code=N2VhZjI3ZGI1Y2JiMjA5YTE3Y2NlN2ViMmQ0Y2Q3OThfUzVKWVpTSkJ1TjNxdGFTcEJEa1JmU0MxeEM3ZHk2VTRfVG9rZW46SFI1VmJHQUMwb3gzT2R4VzR4MmNLOWJkblljXzE3NjUyNTczMjA6MTc2NTI2MDkyMF9WNA" alt/></p>
<p>还有相应的损失函数和优化器等：</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>from torchmetrics.classification import Accuracy</span></span>
<span data-line><span>acc_fn = Accuracy(task=&quot;multiclass&quot;, num_classes=10).to(device)</span></span>
<span data-line> </span>
<span data-line><span>loss_fn = nn.CrossEntropyLoss()</span></span>
<span data-line><span>optimizer = torch.optim.SGD(params=model.parameters(), </span></span>
<span data-line><span>                             lr=0.1)</span></span></code></pre></figure>
<p>万事俱备，我们就可以开始训练了，有了前面训练步骤和测试步骤两个函数，就是把这些模型、损失函数、准确率函数和优化器给传进去就可以了。换一个模型和相应的这些函数，再传进去，就变成了训练另一个模型。</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>from timeit import default_timer as timer </span></span>
<span data-line> </span>
<span data-line><span>start_time = timer()</span></span>
<span data-line> </span>
<span data-line><span>epochs = 3</span></span>
<span data-line><span>for epoch in range(epochs):</span></span>
<span data-line><span>    print(f&quot;Epoch: {epoch}\n---&quot;)</span></span>
<span data-line><span>    </span></span>
<span data-line><span>    train_step(model=model, </span></span>
<span data-line><span>               data_loader=train_dl, </span></span>
<span data-line><span>               loss_fn=loss_fn, </span></span>
<span data-line><span>               optimizer=optimizer,</span></span>
<span data-line><span>               acc_fn=acc_fn)</span></span>
<span data-line> </span>
<span data-line><span>    test_step(model=model, </span></span>
<span data-line><span>              data_loader=valid_dl,</span></span>
<span data-line><span>              loss_fn=loss_fn,</span></span>
<span data-line><span>              acc_fn=acc_fn)</span></span>
<span data-line> </span>
<span data-line><span>end_time = timer()</span></span></code></pre></figure>
<p><img src="./assets/卷积神经网络CNN/file-20251209131654582.png" alt/></p>
<p>准确率达到了98%，6万张图片过了3遍，只用了17秒。</p>
<h4 id="评估">评估<a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#评估" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h4>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>y_preds = []</span></span>
<span data-line><span>model.eval()</span></span>
<span data-line><span>with torch.inference_mode():</span></span>
<span data-line><span>  for X, y in valid_dl:</span></span>
<span data-line><span>    X, y = X.to(device), y.to(device)</span></span>
<span data-line><span>    y_logit = model(X)</span></span>
<span data-line><span>    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)</span></span>
<span data-line><span>    y_preds.append(y_pred.cpu())</span></span>
<span data-line><span>    </span></span>
<span data-line><span>y_pred_tensor = torch.cat(y_preds)</span></span></code></pre></figure>
<p>把验证数据集传给模型，把预测的结果拿到。我们就可以用前面定义的<code>acc_fn</code>来算一下准确率。</p>
<p><img src="./assets/卷积神经网络CNN/file-20251209131701043.png" alt/></p>
<p>和前面那个测试步骤最后的准确率是一样的，因为我们这里用的同一个数据集，也就是说这个数据集，既拿来测试，又拿来验证。一般情况下，我们可以把数据分成3个数据集，一个训练，一个验证，一个测试。因为这个下载的MNIST数据，本身就是分两块的，懒得再去切分它，就先这样用了。</p>
<p>单纯一个准确率，是不清楚那些分类做得不好的，我们可以用混淆矩阵来看一下。</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>from torchmetrics import ConfusionMatrix</span></span>
<span data-line><span>confmat = ConfusionMatrix(num_classes=10, task='multiclass')</span></span>
<span data-line><span>confmat_tensor = confmat(preds=y_pred_tensor, target=y_valid)</span></span></code></pre></figure>
<p>这个代码就计算出了混淆矩阵，我们再画个图来看：</p>
<figure data-rehype-pretty-code-figure><pre tabindex="0" data-language="Python" data-theme="github-light github-dark"><code data-language="Python" data-theme="github-light github-dark" style="display:grid;"><span data-line><span>from mlxtend.plotting import plot_confusion_matrix</span></span>
<span data-line> </span>
<span data-line><span>fig, ax = plot_confusion_matrix(</span></span>
<span data-line><span>    conf_mat = confmat_tensor.numpy(),</span></span>
<span data-line><span>    class_names = range(10)</span></span>
<span data-line><span>)</span></span></code></pre></figure>
<p><img src="./assets/卷积神经网络CNN/file-20251209131706505.png" alt/></p></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div class="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false,&quot;enableRadial&quot;:false}"></div><button class="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div class="global-graph-outer"><div class="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.2,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true,&quot;enableRadial&quot;:true}"></div></div></div><div class="toc desktop-only"><button type="button" class="toc-header" aria-controls="toc-12" aria-expanded="true"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><ul id="list-1" class="toc-content overflow"><li class="depth-0"><a href="#cnn概述" data-for="cnn概述">CNN概述</a></li><li class="depth-0"><a href="#卷积层" data-for="卷积层">卷积层</a></li><li class="depth-1"><a href="#滤波器filterkernel" data-for="滤波器filterkernel">滤波器（filter/kernel）</a></li><li class="depth-1"><a href="#填充padding" data-for="填充padding">填充（padding）</a></li><li class="depth-1"><a href="#步幅stride" data-for="步幅stride">步幅（stride）</a></li><li class="depth-0"><a href="#池化" data-for="池化">池化</a></li><li class="depth-0"><a href="#pytorch实战" data-for="pytorch实战">PyTorch实战</a></li><li class="depth-1"><a href="#数据" data-for="数据">数据</a></li><li class="depth-1"><a href="#模型" data-for="模型">模型</a></li><li class="overflow-end"></li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.5.2</a> © 2026</p><p>Content licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0</a></p><ul><li><a href="https://github.com/jackyzha0/quartz">GitHub</a></li><li><a href="https://discord.gg/cRFFHYye7t">Discord Community</a></li></ul></footer></div></div></body><script type="application/javascript" data-persist="true">function n(){let t=this.parentElement;t.classList.toggle("is-collapsed");let e=t.getElementsByClassName("callout-content")[0];if(!e)return;let l=t.classList.contains("is-collapsed");e.style.gridTemplateRows=l?"0fr":"1fr"}function c(){let t=document.getElementsByClassName("callout is-collapsible");for(let e of t){let l=e.getElementsByClassName("callout-title")[0],s=e.getElementsByClassName("callout-content")[0];if(!l||!s)continue;l.addEventListener("click",n),window.addCleanup(()=>l.removeEventListener("click",n));let o=e.classList.contains("is-collapsed");s.style.gridTemplateRows=o?"0fr":"1fr"}}document.addEventListener("nav",c);
</script><script type="module" data-persist="true">function E(a,e){if(!a)return;function t(o){o.target===this&&(o.preventDefault(),o.stopPropagation(),e())}function n(o){o.key.startsWith("Esc")&&(o.preventDefault(),e())}a?.addEventListener("click",t),window.addCleanup(()=>a?.removeEventListener("click",t)),document.addEventListener("keydown",n),window.addCleanup(()=>document.removeEventListener("keydown",n))}function f(a){for(;a.firstChild;)a.removeChild(a.firstChild)}var m=class{constructor(e,t){this.container=e;this.content=t;this.setupEventListeners(),this.setupNavigationControls(),this.resetTransform()}isDragging=!1;startPan={x:0,y:0};currentPan={x:0,y:0};scale=1;MIN_SCALE=.5;MAX_SCALE=3;cleanups=[];setupEventListeners(){let e=this.onMouseDown.bind(this),t=this.onMouseMove.bind(this),n=this.onMouseUp.bind(this),o=this.onTouchStart.bind(this),r=this.onTouchMove.bind(this),i=this.onTouchEnd.bind(this),s=this.resetTransform.bind(this);this.container.addEventListener("mousedown",e),document.addEventListener("mousemove",t),document.addEventListener("mouseup",n),this.container.addEventListener("touchstart",o,{passive:!1}),document.addEventListener("touchmove",r,{passive:!1}),document.addEventListener("touchend",i),window.addEventListener("resize",s),this.cleanups.push(()=>this.container.removeEventListener("mousedown",e),()=>document.removeEventListener("mousemove",t),()=>document.removeEventListener("mouseup",n),()=>this.container.removeEventListener("touchstart",o),()=>document.removeEventListener("touchmove",r),()=>document.removeEventListener("touchend",i),()=>window.removeEventListener("resize",s))}cleanup(){for(let e of this.cleanups)e()}setupNavigationControls(){let e=document.createElement("div");e.className="mermaid-controls";let t=this.createButton("+",()=>this.zoom(.1)),n=this.createButton("-",()=>this.zoom(-.1)),o=this.createButton("Reset",()=>this.resetTransform());e.appendChild(n),e.appendChild(o),e.appendChild(t),this.container.appendChild(e)}createButton(e,t){let n=document.createElement("button");return n.textContent=e,n.className="mermaid-control-button",n.addEventListener("click",t),window.addCleanup(()=>n.removeEventListener("click",t)),n}onMouseDown(e){e.button===0&&(this.isDragging=!0,this.startPan={x:e.clientX-this.currentPan.x,y:e.clientY-this.currentPan.y},this.container.style.cursor="grabbing")}onMouseMove(e){this.isDragging&&(e.preventDefault(),this.currentPan={x:e.clientX-this.startPan.x,y:e.clientY-this.startPan.y},this.updateTransform())}onMouseUp(){this.isDragging=!1,this.container.style.cursor="grab"}onTouchStart(e){if(e.touches.length!==1)return;this.isDragging=!0;let t=e.touches[0];this.startPan={x:t.clientX-this.currentPan.x,y:t.clientY-this.currentPan.y}}onTouchMove(e){if(!this.isDragging||e.touches.length!==1)return;e.preventDefault();let t=e.touches[0];this.currentPan={x:t.clientX-this.startPan.x,y:t.clientY-this.startPan.y},this.updateTransform()}onTouchEnd(){this.isDragging=!1}zoom(e){let t=Math.min(Math.max(this.scale+e,this.MIN_SCALE),this.MAX_SCALE),n=this.content.getBoundingClientRect(),o=n.width/2,r=n.height/2,i=t-this.scale;this.currentPan.x-=o*i,this.currentPan.y-=r*i,this.scale=t,this.updateTransform()}updateTransform(){this.content.style.transform=`translate(${this.currentPan.x}px, ${this.currentPan.y}px) scale(${this.scale})`}resetTransform(){let t=this.content.querySelector("svg").getBoundingClientRect(),n=t.width/this.scale,o=t.height/this.scale;this.scale=1,this.currentPan={x:(this.container.clientWidth-n)/2,y:(this.container.clientHeight-o)/2},this.updateTransform()}},T=["--secondary","--tertiary","--gray","--light","--lightgray","--highlight","--dark","--darkgray","--codeFont"],y;document.addEventListener("nav",async()=>{let e=document.querySelector(".center").querySelectorAll("code.mermaid");if(e.length===0)return;y||=await import("https://cdnjs.cloudflare.com/ajax/libs/mermaid/11.4.0/mermaid.esm.min.mjs");let t=y.default,n=new WeakMap;for(let r of e)n.set(r,r.innerText);async function o(){for(let s of e){s.removeAttribute("data-processed");let c=n.get(s);c&&(s.innerHTML=c)}let r=T.reduce((s,c)=>(s[c]=window.getComputedStyle(document.documentElement).getPropertyValue(c),s),{}),i=document.documentElement.getAttribute("saved-theme")==="dark";t.initialize({startOnLoad:!1,securityLevel:"loose",theme:i?"dark":"base",themeVariables:{fontFamily:r["--codeFont"],primaryColor:r["--light"],primaryTextColor:r["--darkgray"],primaryBorderColor:r["--tertiary"],lineColor:r["--darkgray"],secondaryColor:r["--secondary"],tertiaryColor:r["--tertiary"],clusterBkg:r["--light"],edgeLabelBackground:r["--highlight"]}}),await t.run({nodes:e})}await o(),document.addEventListener("themechange",o),window.addCleanup(()=>document.removeEventListener("themechange",o));for(let r=0;r<e.length;r++){let v=function(){let g=l.querySelector("#mermaid-space"),h=l.querySelector(".mermaid-content");if(!h)return;f(h);let w=i.querySelector("svg").cloneNode(!0);h.appendChild(w),l.classList.add("active"),g.style.cursor="grab",u=new m(g,h)},M=function(){l.classList.remove("active"),u?.cleanup(),u=null},i=e[r],s=i.parentElement,c=s.querySelector(".clipboard-button"),d=s.querySelector(".expand-button"),p=window.getComputedStyle(c),L=c.offsetWidth+parseFloat(p.marginLeft||"0")+parseFloat(p.marginRight||"0");d.style.right=`calc(${L}px + 0.3rem)`,s.prepend(d);let l=s.querySelector("#mermaid-container");if(!l)return;let u=null;d.addEventListener("click",v),E(l,M),window.addCleanup(()=>{u?.cleanup(),d.removeEventListener("click",v)})}});
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript" data-persist="true"></script><script src="../../postscript.js" type="module" data-persist="true"></script></html>