{"MyPackages/clusterProfiler/KEGG的分类数据更新":{"slug":"MyPackages/clusterProfiler/KEGG的分类数据更新","filePath":"MyPackages/clusterProfiler/KEGG的分类数据更新.md","title":"KEGG的分类数据更新","links":[],"tags":["clusterProfiler","KEGG"],"content":"Dear clusterProfiler maintainers,\nFirst, thank you for developing and maintaining this incredibly useful package! I’ve been using clusterProfiler extensively for enrichment analysis and it’s been invaluable for my research.\nI’m encountering an issue with KEGG enrichment analysis where some pathways show NA values for both category and subcategory fields, even though the pathways themselves are valid and statistically significant.\nDescription\nWhen performing KEGG enrichment analysis using enrichKEGG followed by setReadable, some pathways show NA values for both category and subcategory fields, even though the pathways themselves are valid and significant.\nProblem\nThe pathways mmu04082 (Neuroactive ligand-receptor interaction) and mmu04517 (IgSF CAM signaling) return NA for category and subcategory columns in the results, while other pathways have proper classification information.\nQuestions\nIs this a known issue with specific KEGG pathways?\nIs there a way to automatically retrieve the missing category information?\nShould the category information be populated from a different source or database?\nI’ve verified that these pathways exist in the KEGG database and are valid, but the classification metadata seems to be missing in the enrichment results.\nAny guidance on how to resolve this would be greatly appreciated. Thank you for your time and continued support of this excellent package!\nBest regards,\nSophia\nformula &lt;- compareCluster(ENTREZID~cluster, data=GeneClusterDF,\n                          fun=&#039;enrichKEGG&#039;, \n                          organism = &quot;mmu&quot;,  # 小鼠的KEGG organism code\n                          pvalueCutoff=0.05,\n                          pAdjustMethod = &quot;BH&quot;)\nformula &lt;- setReadable(formula, \n                       OrgDb = &quot;org.Mm.eg.db&quot;, \n                       keyType = &quot;ENTREZID&quot;)\nhead(as.data.frame(formula))\n&gt; head(as.data.frame(formula))\n  Cluster cluster                             category         subcategory       ID                  Description GeneRatio   BgRatio RichFactor FoldEnrichment\n1       0       0                                 &lt;NA&gt;                &lt;NA&gt; mmu04082 Neuroactive ligand signaling    14/113 196/10632 0.07142857       6.720607\n2       0       0                                 &lt;NA&gt;                &lt;NA&gt; mmu04517           IgSF CAM signaling    16/113 300/10632 0.05333333       5.018053\n3       0       0                   Organismal Systems      Nervous system mmu04724        Glutamatergic synapse    10/113 117/10632 0.08547009       8.041752\n4       0       0 Environmental Information Processing Signal transduction mmu04020    Calcium signaling pathway    13/113 255/10632 0.05098039       4.796668\n5       0       0                   Organismal Systems      Nervous system mmu04727            GABAergic synapse     8/113  91/10632 0.08791209       8.271516\n6       0       0 Environmental Information Processing Signal transduction mmu04024       cAMP signaling pathway    11/113 224/10632 0.04910714       4.620417\n    zScore       pvalue     p.adjust       qvalue                                                                                              geneID Count\n1 8.378035 1.822353e-08 4.355424e-06 3.356966e-06          Adcy2/Gabbr2/Gabra2/Gabrb1/Gnao1/Gria2/Grin2c/Grm3/Hrh1/Plcb1/Slc1a2/Slc1a3/Slc6a1/Slc6a11    14\n2 7.316833 1.016818e-07 1.215097e-05 9.365424e-06 Actr3b/Ank2/Cables1/Cadm1/Cadm2/Cntn1/Kcnq3/Kirrel3/Lrrc4c/Mapk10/Ncam1/Nrcam/Nrp1/Srgap1/Tjp1/Vav3    16\n3 7.937944 4.238134e-07 3.376380e-05 2.602363e-05                                        Adcy2/Glul/Gnao1/Gria2/Grin2c/Grm3/Itpr2/Plcb1/Slc1a2/Slc1a3    10\n4 6.360275 2.911406e-06 1.739565e-04 1.340779e-04                     Adcy2/Atp2b2/Camk1d/Camk2g/Fgf1/Fgfr3/Grin2c/Hrh1/Itpr2/Phka1/Phkg1/Plcb1/Vegfa    13\n5 7.220110 5.181671e-06 2.476839e-04 1.909037e-04                                                Adcy2/Gabbr2/Gabra2/Gabrb1/Glul/Gnao1/Slc6a1/Slc6a11     8\n6 5.675943 2.505014e-05 9.978306e-04 7.690833e-04                               Adcy2/Atp1a2/Atp2b2/Camk2g/Gabbr2/Gli3/Gria2/Grin2c/Mapk10/Ptch1/Vav3    11\n&gt; sessionInfo()\nR version 4.5.2 (2025-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64\nRunning under: Windows 11 x64 (build 22000)\n \nMatrix products: default\n  LAPACK version 3.12.1\n \nlocale:\n[1] LC_COLLATE=Chinese (Simplified)_China.utf8  LC_CTYPE=Chinese (Simplified)_China.utf8    LC_MONETARY=Chinese (Simplified)_China.utf8\n[4] LC_NUMERIC=C                                LC_TIME=Chinese (Simplified)_China.utf8    \n \ntime zone: Asia/Shanghai\ntzcode source: internal\n \nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods   base     \n \nother attached packages:\n [1] dplyr_1.1.4            ReactomePA_1.54.0      GseaVis_0.0.5          pathview_1.50.0        DOSE_4.4.0             enrichplot_1.30.0     \n [7] ggplot2_4.0.1          msigdbr_25.1.1         org.Mm.eg.db_3.22.0    AnnotationDbi_1.72.0   IRanges_2.44.0         S4Vectors_0.48.0      \n[13] Biobase_2.70.0         BiocGenerics_0.56.0    generics_0.1.4         clusterProfiler_4.18.0\n \nloaded via a namespace (and not attached):\n  [1] pheatmap_1.0.13         pak_0.9.0               DBI_1.2.3               httr_1.4.7              BiocParallel_1.44.0     yulab.utils_0.2.1      \n  [7] ggplotify_0.1.3         babelgene_22.9          pillar_1.11.1           Rgraphviz_2.54.0        R6_2.6.1                mime_0.13              \n [13] reticulate_1.44.1       viridis_0.6.5           ROCR_1.0-11             graphite_1.56.0         S7_0.2.1                parallelly_1.45.1      \n [19] GlobalOptions_0.1.2     polyclip_1.10-7         htmltools_0.5.8.1       remotes_2.5.0           ggrepel_0.9.6           fgsea_1.36.0           \n [25] forcats_1.0.1           spatstat.utils_3.2-0    fitdistrplus_1.2-4      tidyselect_1.2.1        utf8_1.2.6              RSQLite_2.4.4          \n [31] cowplot_1.2.0           scattermore_1.2         sessioninfo_1.2.3       spatstat.data_3.1-9     gridExtra_2.3           fs_1.6.6               \n [37] sctransform_0.4.2       RColorBrewer_1.1-3      future.apply_1.20.0     graph_1.88.0            R.oo_1.27.1             RcppHNSW_0.6.0         \n [43] reactome.db_1.94.0      Rtsne_0.17              lazyeval_0.2.2          scales_1.4.0            treeio_1.34.0           R.utils_2.13.0         \n [49] KEGGgraph_1.70.0        bitops_1.0-9            R.methodsS3_1.8.2       KEGGREST_1.50.0         promises_1.5.0          shape_1.4.6.1          \n [55] zoo_1.8-14              RSpectra_0.16-2         assertthat_0.2.1        tools_4.5.2             ape_5.8-1               shiny_1.11.1           \n [61] rlang_1.1.6             ggridges_0.5.7          evaluate_1.0.5          otel_0.2.0              reshape2_1.4.5          devtools_2.4.6         \n [67] colorspace_2.1-2        ellipsis_0.3.2          data.table_1.17.8       withr_3.0.2             tibble_3.3.0            RCurl_1.98-1.17        \n [73] xtable_1.8-4            plyr_1.8.9              aplot_0.2.9             systemfonts_1.3.1       httpuv_1.6.16           MASS_7.3-65            \n [79] stringr_1.6.0           openxlsx_4.2.8.1        GO.db_3.22.0            vctrs_0.6.5             lifecycle_1.0.4         codetools_0.2-20       \n [85] fastDummies_1.7.5       nlme_3.1-168            Seqinfo_1.0.0           future_1.68.0           pkgload_1.4.1           Rcpp_1.1.0             \n [91] rstudioapi_0.17.1       patchwork_1.3.2         stringi_1.8.7           pbapply_1.7-4           cachem_1.1.0            BiocManager_1.30.27    \n [97] tidytree_0.4.6          listenv_0.10.0          XVector_0.50.0          plotly_4.11.0           ggtree_4.0.1            pkgbuild_1.4.8         \n[103] ggfun_0.2.0             ggtangle_0.0.8          htmlwidgets_1.6.4       memoise_2.0.1           crayon_1.5.3            gridGraphics_0.5-1     \n[109] rappdirs_0.3.3          GOSemSim_2.36.0         png_0.1-8               progressr_0.18.0        fastmap_1.2.0           tidygraph_1.3.1        \n[115] tidyr_1.3.1             pkgconfig_2.0.3         cli_3.6.5               ggforce_0.5.0           ggiraph_0.9.2           lmtest_0.9-40          \n[121] usethis_3.2.1           RcppAnnoy_0.0.22        gdtools_0.4.4           viridisLite_0.4.2       splines_4.5.2           blob_1.2.4             \n[127] XML_3.99-0.20           globals_0.18.0          knitr_1.50              ica_1.0-3               spam_2.11-1             dichromat_2.0-0.1      \n[133] compiler_4.5.2          grid_4.5.2              bit_4.6.0               ggpp_0.5.9              glue_1.8.0              sp_2.2-0               \n[139] digest_0.6.38           irlba_2.3.5.1           graphlayouts_1.2.2      fontLiberation_0.1.0    fontBitstreamVera_0.1.1 dotCall64_1.2          \n[145] tweenr_2.0.3            lattice_0.22-7          ggraph_2.2.2            gson_0.1.0              igraph_2.2.1            ggnewscale_0.5.2       \n[151] qvalue_2.42.0           later_1.4.4             parallel_4.5.2          fontquiver_0.2.1        miniUI_0.1.2            gtable_0.3.6           \n[157] xfun_0.54               Biostrings_2.78.0       curl_7.0.0              org.Hs.eg.db_3.22.0     KernSmooth_2.23-26      survival_3.8-3         \n[163] jsonlite_2.0.0          magrittr_2.0.4          purrr_1.2.0             matrixStats_1.5.0       Matrix_1.7-4            SeuratObject_5.2.0     \n[169] fastmatch_1.1-6         RANN_2.6.2              circlize_0.4.16         polynom_1.4-1           bit64_4.6.0-1           cluster_2.1.8.1        \n[175] farver_2.1.2            zip_2.3.3  \n来自用户的灵魂拷问，连着三个问句，这必须是有经历过探索的。我看了一下，这个KEGG category的映射关系是预存在包里的，然后在Bioconductor 3.22新发行版的时候，我忘记去更一下数据了。\n那么KEGG本来就是在线抓取数据，为什么这个要预存数据呢？也在线爬它不香吗？这个我肯定是有原因的。\n\n\n因为这个category，不管你是什么物种，都是这样的分类，所以它是通用的。所有人一样用，这就适合存一份。\n\n\n另一方面，爬这个category的信息，我用的rvest去写的爬虫，我不想放到clusterProfiler的代码里，因为放进去，就会增加依赖包。\n\n\n我给用户的回复：\nThe KEGG pathway information is cached within the package. I apologize for not updating it before the latest release. Thank you for bringing this to my attention; the issue has now been resolved.\n&gt; head(x, 2)\n                                     category                         subcategory       ID                  Description GeneRatio   BgRatio RichFactor FoldEnrichment   zScore       pvalue     p.adjust       qvalue\nmmu04082 Environmental Information Processing Signaling molecules and interaction mmu04082 Neuroactive ligand signaling     30/30 196/10632  0.1530612       54.24490 40.02145 9.349530e-54 1.084545e-51 4.527141e-52\nmmu05032                       Human Diseases                Substance dependence mmu05032           Morphine addiction     13/30  93/10632  0.1397849       49.53978 25.00883 7.801411e-20 4.524818e-18 1.888763e-18\n                                                                                                                                                                                             geneID Count\nmmu04082 216227/11423/14678/14701/11549/14654/18750/57385/11539/15559/14683/11515/11541/104111/18442/242425/14806/13491/21334/63993/13488/213788/57014/108015/14396/21337/53623/110886/11513/210044    30\nmmu05032  \n昨晚给我的留言，早上就许愿成功。\n\n这个锅我是认的。\n\n\n正如前文所的，预存数据是有原因的，但这也带来了要定期更新的问题。因为这才更没多久，又来反映没更了。这样子手工更，就太费叔了，一下子把自己搞手残了。\n所谓那里有压迫，那里就有反抗，不能让它把我们给欺负了。\n我的解决方案就是用github action。写个workflow来干这个事情。\n\n触发条件 ：每周日 UTC 时间 00:00 ( cron: ‘0 0 * * 0’ ) 或手动触发 ( workflow_dispatch )。\n环境配置 ：自动安装 R 环境和必要的依赖（包括 rvest , tibble , tidyr 以及 clusterProfiler 自身）。\n执行更新 ：安装当前包后，完全按照我写好的 make updatedata 逻辑运行。\n自动 PR ：如果有文件变动（ .rda 文件更新），自动创建一个名为 update-kegg-data 的分支并提交 PR。\n\n我手动触发，试一下它work不work。显然是没有问题的，pull request已经正常出现。那么我只要merge就行了。做到数据每周定期检查，如有更新，就自动更新。\n\n好了，活人终究不能被尿憋死。\n后续\n终究我还是懒，KEGG更新还挺频繁的，我不想手工merge，再push到Bioconductor。于是数据和包分离，数据是预存没错，但不放在包里，而是放到github上，然后GitHub Actions自动更新。包里不带数据，总是从github上获取，这样最终可以做到不用管。"},"MyPackages/enrichplot/gseaplot2中的pvalue_table定制化":{"slug":"MyPackages/enrichplot/gseaplot2中的pvalue_table定制化","filePath":"MyPackages/enrichplot/gseaplot2中的pvalue_table定制化.md","title":"gseaplot2中的pvalue_table定制化","links":[],"tags":["enrichplot","GSEA","visualization"],"content":"\n来自于GitHub中的需求，两个问题：\n\n\n只画一条通路，不想要pvalue_table的rowname\n\n\n希望pvalue_table可定制，比如可以把NES放进去。\n\n\n先跑个示例：\nlibrary(DOSE)\ndata(geneList)\nx &lt;- gseDO(geneList)\n那么我在gseaplot2这个函数里加了两个参数，分别应对这两个问题：\n\n\npvalue_table_columns：用于指定什么信息放进表格里展示\n\n\npvalue_table_rownames：用于指定那个信息拿来当rownames\n\n\n默认行为保持不变：\ngseaplot2(x, 1, pvalue_table=T)\n\n去掉rownames，只需将pvalue_table_rownames设为NULL:\ngseaplot2(x, 1, pvalue_table=T, pvalue_table_rownames=NULL)\n\n可以指定rownames用什么信息，比如此处用ID，表格通过pvalue_table_columns可以自由指定：\ngseaplot2(x, 1, pvalue_table=T, \n    pvalue_table_rownames=&quot;ID&quot;, \n    pvalue_table_columns=c(&quot;NES&quot;, &quot;p.adjust&quot;))\n\n像上面这个图，也可以不要rownames，然后把这个信息放到table里：\ngseaplot2(x, 1, pvalue_table=T, \n    pvalue_table_rownames=NULL, \n    pvalue_table_columns=c(&quot;ID&quot;, &quot;NES&quot;, &quot;p.adjust&quot;))\n"},"MyPackages/enrichplot/p值的分布你考虑过没有？":{"slug":"MyPackages/enrichplot/p值的分布你考虑过没有？","filePath":"MyPackages/enrichplot/p值的分布你考虑过没有？.md","title":"p值的分布你考虑过没有？","links":[],"tags":["enrichplot","visualization","pvalue"],"content":"画图拿p值来上色，基本上很多人没有注意p值的分布，除非是工具培养了习惯，比如说火山图，p值就被转换成-10*log10(pvalue)，所有人就默认这样画，但换一个就不清不楚了。\n如果是一个随机的东西，去做检验，那p值的分布也会相对比较均一，或者是比较正态，均值在0.5附近之类的。\n这个我们都可以试一下的，拿随机的基因ID来搞个富集分析：\nrequire(clusterProfiler)\ndata(geneList, package=&quot;DOSE&quot;)\n \nset.seed(42)\nde2 = sample(names(geneList), 200)\n \ny = enrichGO(de2, OrgDb=&#039;org.Hs.eg.db&#039;)\nplot(density(y@result$p.adjust, bw=0.01))\n\n我们拿个差异基因来做富集的话，结果是不一样的。\nde &lt;- names(geneList)[1:200]\nx = enrichGO(de, OrgDb=&#039;org.Hs.eg.db&#039;)\nplot(density(x@result$pvalue, bw=0.01))\n\n就会有比较多的很小的p值，是一个偏态分布。\n比如我拿前面20条通路来画图，我们看一下p值的分布：\nplot(x@result$pvalue[1:20])\n\n这样拿去映射颜色的话，比如说红-蓝，结果就是尾部几个p值大点的是蓝色，其它的一片红色，区分不开。\n我们取个对数看看：\nplot(log10(x@result$pvalue[1:20]))\n\n区分度就有了。我们从整体上看的话，p值小的就被拉开了，p值大的，会聚集。\nplot(density(log10(x@result$pvalue), bw=0.01))\n\n所以如果只是单纯地把p值映射到颜色，就会是下面这两个图这样子，一片红区分不开。这样的图相信大家经常看到。\n\n\n所以我自己画图的时候，我是会对p值取对数来映射的。你看看我们2021年在The Innovation上发的clusterProfiler 4.0的文章，图全部是我操刀的。\n\n\n这些图的颜色区分度是有的。而画图的代码我是有提供的。大家可能没注意到这个细节，也就没有去抄我的代码。\n对于上面的x对象，你用dotplot()来画，出来的图是这样子的：\ndotplot(x)\n\n就是这样子红点多，且没有区分度。\n你是可以进行变换的。\ndotplot(x) + set_enrichplot_color(type=&#039;fill&#039;, transform=&#039;log10&#039;) \n出来的图就是这样子，有区分度了。\n\n这个功能很多人可能不知道。enrichplot中不同的图，配色一致，我不想要在不同的函数里有差不多的设置颜色代码，于是就写了这个set_enrichplot_color来统一设置颜色。它还是蛮好用的。比如说我想把颜色换成以前经典的红-蓝配色。只需要指定colors=c(&quot;red&quot;, &quot;blue&quot;)即可。\ndotplot(x) + set_enrichplot_color(type=&#039;fill&#039;, transform=&#039;log10&#039;, colors=c(&quot;red&quot;, &quot;blue&quot;))\n\n虽然长期以来，我们是可以对p值取对数的，但太多人不会用了，太多的图，都是一片红了。于是我决定在新的版本中，让取对数成为默认。所以如果你用了v&gt;=1.29.2，那么你用dotplot(x)出来的，就是取了对数的p值进行颜色映射的。\n你可能又会想，你全都要，新的默认你想要，旧的默认，你还想要，怎么办？简单，还是用set_enrichplot_color，你只要把transform=&quot;identity&quot;传入即可。也就是：\ndotplot(x) + set_enrichplot_color(type=&#039;fill&#039;, transform=&#039;identity&#039;) "},"MyPackages/ggtree/可交互的ggtree":{"slug":"MyPackages/ggtree/可交互的ggtree","filePath":"MyPackages/ggtree/可交互的ggtree.md","title":"可交互的ggtree","links":[],"tags":["ggtree","interactive"],"content":"\n\n开发ggtree就是我的使命！\n\n可交互这个功能是GISAID的创始人Peter Bogner和我提出来的，我们拉了个线上Zoom会议，聊了聊。于是我和双斌就着手来做这下事情。\n当我们做了个demo，发给他们的时候，跟我说，两周后WHO要开会选疫苗株，他们把数据整理了发我，我来帮他们画。\n\n这时候，我基本上就有求必应，乃至于说加班加点也要帮他们弄了。因为可以说，这是在给世界做贡献。\nggtree的交互，出现得悄无声息\nlibrary(ggtree)\n \n  \nset.seed(123)\ntr &lt;- rtree(20)\n# 这是静态的，和原来一样\np &lt;- ggtree(tr)\n \n# 交互开关，这第二个p，是可交互的图了\nggtree_set_interactive()\np\n\n就是这么简单，这是一个截图，所以看不出啥，但我们从红色框出来的地方，可以看到，这是一张html的【可交互】的图。\n当然这张图，事实上我们没有任何可交互的设置，它其实是没有交互元素的可交互图，有点绕口，但差不多可以说，搞了个寂寞。\n但它就是这么简单，有个开关，你开了，是交互的，关了，是静态的。你感觉不到它的存在。这就是我们所追求的，极致的简单，没有学习成本。\n就这个事情，双斌给我提了意见，说搞个寂寞这种，要给个warning。\n\n我给他的回复就是，搞个寂寞在某个场景下，或者是有用的。我再加个开关，可以关掉warning。\n\n没错，我们就是这么细节控，这样才做到了，呈现给大家的是极致的简单。\n怎么样可交互？\n你基本上只需要知道，加多了两个映射，就可以了：\n\ndata_id 为图形元素分配唯一标识符，用于JavaScript回调函数的元素识别，实现交互功能。\ntooltip 为图形元素添加鼠标悬停提示框，当鼠标悬停时，显示自定义信息。\n\n核心就这些，其它的以后再讲。\n让我们来看一个真正的可交互图：\np2 &lt;- ggtree(\n       tr,\n       mapping = aes(\n           tooltip = round(branch.length, 2),\n           data_id = node\n         )\n   )\n \np2\n代码简单吧，显示的信息是保留两个小数点的分支长度。\n\n鼠标悬停，信息出现，就这样简单。\n然后我们照常加图层，你可以在多个图层都设置有悬停，都是可以的。\ndt &lt;- data.frame(id = c(36, 38), type=c(&quot;A&quot;, &quot;B&quot;))\n \np3 &lt;- p2 +\n     geom_hilight(\n        data = dt,\n        mapping = aes(\n           node = id,\n           fill = type,\n           tooltip = paste0(&quot;clade of node &quot;, id),\n           data_id = type\n        ),\n        to.bottom = TRUE\n     )\n \np3\n\n眼尖的你可能发现，这张截图，高亮的区域和legend对应不上，其实是对着的，没有问题。你看到的不一样，是因为我截图的是鼠标正好悬停在上面，鼠标悬停会高亮相应的图形元素，就像上一张图，在分支上，那个分支就有了颜色一样，所以相当于叠多了个颜色，就看着不一样。当我们把鼠标移开之后，它就是原来的颜色，是一模一样的。\n然后可交互图，一般不是只给你导出PNG, SVG这些，不给保存PDF？\n让我们把交互特性给关了。\nggtree_unset_interactive()\n \np3\n\n这就是美妙之处，零学习成本，静态与交互，随意切换。"},"MyPackages/mirrorselect/包不存在！镜像要不要背锅？":{"slug":"MyPackages/mirrorselect/包不存在！镜像要不要背锅？","filePath":"MyPackages/mirrorselect/包不存在！镜像要不要背锅？.md","title":"包不存在！镜像要不要背锅？","links":[],"tags":["mirrorselect","R"],"content":"一条指令装一下ggplot2，它竟然告诉我没这个包，这难道不是在欺负我？\n&gt; install.packages(&quot;ggplot2&quot;)\nInstalling package into ‘E:/software-data/RLibrary’\n(as ‘lib’ is unspecified)\nWarning: unable to access index for repository mirrors.ustc.edu.cn/CRAN/src/contrib:\n  cannot open URL &#039;mirrors.ustc.edu.cn/CRAN/src/contrib/PACKAGES&#039;\nWarning: unable to access index for repository mirrors.ustc.edu.cn/CRAN/bin/windows/contrib/4.5:\n  cannot open URL &#039;mirrors.ustc.edu.cn/CRAN/bin/windows/contrib/4.5/PACKAGES&#039;\nWarning message:\npackage ‘ggplot2’ is not available for this version of R\n \nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\ncran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages \n怀疑人生，想起我6年前写的魔镜mirrorselect包（电梯：《魔镜魔镜告诉你，它上CRAN了》），果断测速，换个镜像。\n&gt; mirrorselect::get_mirror(&#039;CRAN&#039;, &#039;cn&#039;) -&gt; m\n&gt; head(m)\n[1] &quot;mirrors.tuna.tsinghua.edu.cn/CRAN/&quot; &quot;mirrors.bfsu.edu.cn/CRAN/&quot;          &quot;mirrors.pku.edu.cn/CRAN/&quot;          \n[4] &quot;mirrors.ustc.edu.cn/CRAN/&quot;          &quot;mirrors.zju.edu.cn/CRAN/&quot;           &quot;mirrors.qlu.edu.cn/CRAN/&quot;          \n&gt; length(m)\n[1] 12\n&gt; mirrorselect::mirrorselect(m) -&gt; st\n&gt; st\n                                       mirror speed\n1           mirrors.hust.edu.cn/CRAN/  0.13\n2           mirrors.ustc.edu.cn/CRAN/  0.14\n3        mirrors.sustech.edu.cn/CRAN/  0.14\n4            mirrors.pku.edu.cn/CRAN/  0.18\n5            mirrors.nju.edu.cn/CRAN/  0.23\n6             mirror.lzu.edu.cn/CRAN/  0.26\n7     mirrors.sjtug.sjtu.edu.cn/cran/  0.27\n8          mirrors.nwafu.edu.cn/cran/  0.28\n9            mirrors.zju.edu.cn/CRAN/  0.32\n10           mirrors.qlu.edu.cn/CRAN/  0.41\n11          mirrors.bfsu.edu.cn/CRAN/  0.53\n12 mirrors.tuna.tsinghua.edu.cn/CRAN/  0.55\n看上去华科最快，换上再试试，装包就丝滑了。\n&gt; options(repos = c(CRAN = &quot;mirrors.hust.edu.cn/CRAN/&quot;))\n&gt; install.packages(&quot;ggplot2&quot;)\nInstalling package into ‘E:/software-data/RLibrary’\n(as ‘lib’ is unspecified)\ntrying URL &#039;mirrors.hust.edu.cn/CRAN/bin/windows/contrib/4.5/ggplot2_3.5.2.zip&#039;\nContent type &#039;application/zip&#039; length 5016803 bytes (4.8 MB)\ndownloaded 4.8 MB\n \npackage ‘ggplot2’ successfully unpacked and MD5 sums checked\n \nThe downloaded binary packages are in\n        C:\\Users\\HUAWEI\\AppData\\Local\\Temp\\RtmpS8YcTh\\downloaded_packages\n\nCRAN包，用install.packages(&quot;mirrorselect&quot;)就可以安装，让你下包接上大水管。"},"MyPackages/swissmodel/为了上课，给Swiss-Model写个R包吧":{"slug":"MyPackages/swissmodel/为了上课，给Swiss-Model写个R包吧","filePath":"MyPackages/swissmodel/为了上课，给Swiss-Model写个R包吧.md","title":"为了上课 - 给Swiss-Model写个R包吧","links":[],"tags":["swissmodel","R","protein-structure"],"content":"Swiss-Model是啥？\n来自说明文档的介绍：\n\nSWISS-MODEL is a web-based integrated service dedicated to protein structure homology modelling. It guides the user in building protein homology models at different levels of complexity.\nBuilding a homology model comprises four main steps: (i) identification of structural template(s), (ii) alignment of target sequence and template structure(s), (iii) model-building, and (iv) model quality evaluation. These steps require specialised software and integrate up-to-date protein sequence and structure databases. Each of the above steps can be repeated interactively until a satisfying modelling result is achieved.\n\n使用很简单，Web Sever里贴序列就行：\n\n为什么要写个R包？\n上实验课，要使用Swiss-Model进行蛋白质建模，其它的操作都在R里可以搞，但这一步，得到网页上来弄，再把结果下载下来，又回到R。就想问自己一个问题：能不能装逼装全套？于是就着手写一个R包。\n安装\n一条指令安装：\nyulab.utils::install_zip_gh(&quot;YuLab-SMU/swissmodel&quot;)\n设置\n首先是需要一个API token，注册了在账号里就可以看到：\n\n然后加载swissmodel包，用set_swissmodel_token函数设置即可。\nlibrary(swissmodel)\nset_swissmodel_token(&quot;YOUR_SWISSMODEL_TOKEN&quot;)\n建模\n首先你得有序列吧：\nhemoglobin_sequence &lt;- &quot;VLSPADKTNVKAAWAKVGNHAADFGAEALERMFMSFPSTKTYFSHFDLGHNSTQVKGHGKKVADALTKAVGHLDTLPDALSDLSDLHAHKLRVDPVNFKLLSHCLLVTLAAHLPGDFTPSVHASLDKFLASVSTVLTSKYR&quot;\n然后建模直接是写成一个workflow，所以你只要跑这一句代码就行：\nresult &lt;- run_automodel_workflow(hemoglobin_sequence)\n然后就是等，期间函数打印出来的信息如下：\nStarting automodel workflow...\nAUTOMODEL task submitted successfully!\nProject ID: 25f307\nProject status: INITIALISED [Waited: 0 seconds]\nProject status: RUNNING [Waited: 32 seconds]\nProject status: RUNNING [Waited: 63 seconds]\nProject status: RUNNING [Waited: 95 seconds]\nProject status: RUNNING [Waited: 127 seconds]\nProject status: RUNNING [Waited: 158 seconds]\nProject status: RUNNING [Waited: 189 seconds]\nProject status: RUNNING [Waited: 220 seconds]\nProject status: RUNNING [Waited: 251 seconds]\nProject status: RUNNING [Waited: 282 seconds]\nProject status: COMPLETED [Waited: 313 seconds]\nModeling completed successfully!\nFile downloaded successfully: ./modeling_results/25f307_model_1.pdb\nFile downloaded successfully: ./modeling_results/25f307_model_2.pdb\n这个workflow包含了几步：\n\n\n创建一个输出的文件夹，默认./modeling_results\n\n\n提交序列，得到一个项目ID\n\n\n检查这个项目的状态\n\n\n当项目完成之后，拿到结果的URL\n\n\n下载文件\n\n\n返回运行整个workflow的一些相关信息\n\n\nresult里记录了相关的信息，结果如下：\n&gt; result\n$project_id\n[1] &quot;25f307&quot;\n \n$status\n[1] &quot;COMPLETED&quot;\n \n$downloaded_files\n$downloaded_files$model_1\n[1] &quot;./modeling_results/25f307_model_1.pdb&quot;\n \n$downloaded_files$model_2\n[1] &quot;./modeling_results/25f307_model_2.pdb&quot;\n \n$project_info_file\n[1] &quot;./modeling_results/25f307_info.json&quot;\n这些信息也会被写入json文件，这样你退出R之后，这些信息都是有保留的。这个json文件的信息很容易读进来：\n&gt; jsonlite::fromJSON(result$project_info_file)\n$project_id\n[1] &quot;25f307&quot;\n \n$status\n[1] &quot;COMPLETED&quot;\n \n$models\n  model_id    status gmqe                                               coordinates_url                                                  modelcif_url avg_local_score\n1       01 COMPLETED 0.98 swissmodel.expasy.org/project/25f307/models/01.pdb.gz swissmodel.expasy.org/project/25f307/models/01.cif.gz              NA\n2       02 COMPLETED 0.90 swissmodel.expasy.org/project/25f307/models/02.pdb.gz swissmodel.expasy.org/project/25f307/models/02.cif.gz            0.85\n \n$date_created\n[1] &quot;2025-11-10T05:54:31.192032Z&quot;\n \n$project_title\n[1] &quot;R Automodel&quot;\n \n$view_url\n[1] &quot;swissmodel.expasy.org/project/25f307/view&quot;\n这个view_url点开之后，也可以到Swiss-Model网站上看结果：\n\n所以一个流程，完事了。建模完成的pdb文件已经下载到了本地，我们可以在R里读取。\n这个R包到这里，应该说就完成了开头说的，要在R里用Swiss-Model的使命。演示的这个流程，里面每一步都是可以单独运行的，函数都是分开并且是export的。\n蛋白结构初印象\n我还继续写了一些简单的函数，可以做一些简单的分析。\n首先读取PDB文件，调用了bio3d包的read.pdb:\n&gt; pdb = read.pdb(result$downloaded_files[[1]])\n然后你可以用summary来看这个对象的一些信息：\n&gt; summary(pdb)\n \n Call:  read.pdb(file = result$downloaded_files[[1]])\n \n   Total Models#: 1\n     Total Atoms#: 1076,  XYZs#: 3228  Chains#: 1  (values: A)\n \n     Protein Atoms#: 1076  (residues/Calpha atoms#: 141)\n     Nucleic acid Atoms#: 0  (residues/phosphate atoms#: 0)\n \n     Non-protein/nucleic Atoms#: 0  (residues: 0)\n     Non-protein/nucleic resid values: [ none ]\n \n+ attr: atom, xyz, calpha, call\n在swissmodel包里，也提供了其它一些函数，包括pdb_info():\n&gt; pdb_info(pdb)\nProtein Structure Information:\n  - Atoms: 1076\n  - Residues: 141\n  - Chains: A\n  - Model dimensions (&lt;c3&gt;&lt;85&gt;):\n    X: 43.88\n    Y: 27.32\n    Z: 41.17\n$atoms\n[1] 1076\n \n$residues\n[1] 141\n \n$chains\n[1] &quot;A&quot;\n \n$dimensions\n     x      y      z \n43.883 27.317 41.168 \n以及analyze_model_quality():\n&gt; analyze_model_quality(pdb)\n$atoms\n[1] 1076\n \n$residues\n[1] 141\n \n$chains\n[1] &quot;A&quot;\n \n$resolution\n[1] NA\n \n$rama_favored\n[1] 79.28571\n \n$rama_allowed\n[1] 9.285714\n \n$rama_outliers\n[1] 11.42857\n它会计算拉式图的一些参数：\n\n\nRamachandran Favored（最允许区域）：这些区域对应的φ和ψ二面角组合在立体化学上是最允许的，通常对应于蛋白质中常见的二级结构，如α-螺旋和β-折叠。这些区域内的残基构象在能量上是有利的，且没有原子间的空间冲突。\n\n\nRamachandran Allowed（允许区域）：这些区域对应的φ和ψ二面角组合在立体化学上是允许的，但可能不如最允许区域那么理想。这些区域内的残基构象在能量上可能是可接受的，但可能不如最允许区域稳定。\n\n\nRamachandran Outliers（异常区域）：这些区域对应的φ和ψ二面角组合在立体化学上是不允许的，因为它们会导致原子间的空间冲突或能量上不利的构象。出现在这些区域的残基可能表示该残基的构象存在问题，例如在蛋白质结构建模或测定中可能出现了错误。\n\n\n画图\n拉式图（Ramachandran图）来评估蛋白质结构中氨基酸残基的构象是否合理。Ramachandran图是根据蛋白质中每个非末端氨基酸残基的φ和ψ二面角绘制的散点图。这些二面角决定了蛋白质主链的构象。\n也是一条指令：\nplot_ramachandran(pdb)\n\n还可以画一下残基的组成：\nplot_residue_composition(pdb)\n\n最后呢，三维结构不画三维怎么行，还是一条指令：\nplot_pdb(pdb)\n\n这里调用了以前介绍的R包r3dmol。\n对于这些结构简单分析和画图函数，也封装成一个流程，只需要跑run_pdb_analysis()一个函数，就会将这些分析全部跑一遍，然后分别写入文件。\n参考资料\n\nswissmodel.expasy.org/docs/help\n拉氏图: zh.wikipedia.org/wiki/%E6%8B%89%E6%B0%8F%E5%9B%BE\n在R中对分子结构进行3D可视化: mp.weixin.qq.com/s/lOfc6ZRFBI3EWoSgpd5guw\n"},"Tools/Obsidian-Quartz博客系统":{"slug":"Tools/Obsidian-Quartz博客系统","filePath":"Tools/Obsidian-Quartz博客系统.md","title":"Obsidian-Quartz博客系统","links":[],"tags":["blog","quartz","obsidian","github"],"content":"创建Quartz博客\nQuartz是使用Node.js的，所以首先是需要安装Node.js：\nscoop install nodejs-lts\n从github.com/jackyzha0/quartz下载。然后是安装依赖：\nnpm install\n然后就可以创建博客了：\nnpx quartz create\n与Obsidian的联动\n平时用Obsidian来写东西，但并不是写啥，都会公开。所以对于是否要公开，用一个YAML frontmatter做为开关。\n---\npublish: true\n---\n\ntrue是要发表的，false是不发表的。\n但是Quartz默认是不管的，它并不去识别publish: false这个字段，我们需要修改quartz.config.ts，找到这一段：\nimport * as Plugin from &quot;./quartz/plugins&quot;\n \nconst config: QuartzConfig = {\n  configuration: {\n    // ...\n  },\n  plugins: {\n    transformers: [\n      Plugin.FrontMatter(),\n      // ...\n    ],\n    filters: [\n      // 这里通常有一些默认的 filter，比如 RemoveDrafts 之类的\n    ],\n    emitters: [\n      // ...\n    ],\n  },\n}\n \nexport default config\n在filters里面加上Plugin.ExplicitPublish():\nfilters: [\n  Plugin.ExplicitPublish(),\n  // 如果你还有其他 filter，就按需要保留\n  // Plugin.RemoveDrafts(),\n],\n这样子，就会只发布publish: true的markdown文件。\n构建博客\n设置好之后，就可以用obsidian中的文档来生成博客了，用以下指令：\n# 如需要本地预览，加上  --serve\nnpx quartz build -d ../YuNotebooks -o docs\n\n输入直接用上一层目录的obsidian vault，我的笔记在本地，而生成的博客，只有我标识了publish: true的部分，输入的html文档，就放到docs文件夹里，方便后面在GitHub上指定它为Pages。\n但是这里还是有一个问题，就是publish: false的md虽然不会被渲染，但是对应的assets文件夹（放图片，用的Custom Attachment Location）这些还是会被拷贝到docs目录下，必须要写个脚本来清除它，于是就写了一个cleanup_public_no_html.py的脚本。\n然后还有一个问题，就是插入的图片是![](assets/xxx)这种形式，在把md渲染成html的时候，它会把这个目录当成是根目录下的，结果就是md文件如果放一层目录，就会变成../assets/xxx，放两层目录下，就会变成../../assets/xxx，这样子图片就会显示不正常，又需要写一个脚本，correct-image-path.py来读取生成的html文件，修改一下路径。\n发布博客\n到这里就没啥了，推送到GitHub，把docs目录选为Pages，打完收工。\n做个懒人\n还想更进一步，毕竟是和Obsidian联动，就是我只管在Obsidian里写笔记，不用管博客，这是最好的，笔记反正是可以控制是否发表，不需要每次我想发表，就得来弄一下。所以笔记我放到private repo里。搞个Secret，让Actions可以拉取，然后该怎么跑，让Actions去弄。然后我再设置，每天北京时间早上3:00自动跑一次Actions，这样子，我压根不用管博客，只管笔记。笔记也是设置好的，自动同步到GitHub。\n所以整个逻辑就是：\n\n我只管本地写笔记\n后台自动同步GitHub\nActions自动更新博客\n\n然后我只需要做第一步，完美。"},"Tools/见异思迁：扔掉RStudio，拥抱Positron":{"slug":"Tools/见异思迁：扔掉RStudio，拥抱Positron","filePath":"Tools/见异思迁：扔掉RStudio，拥抱Positron.md","title":"use-positron","links":[],"tags":["R","positron","vscode"],"content":"定制版不香吗？\n虽然标题是扔掉RStudio，但我使用RStudio的时间其实非常有限，基本上只有我需要讲课演示的时候，才会安装在课堂上用一下。用了二十年的R，有十几年的时间是在Emacs中使用R的：\n\n后来我一直用一个定制的版本叫SPACEMACS，定制的就是香，省心。\n这几年转向使用VSCODE，平时用着也挺爽的，但是现在有Positron，还是那句话，定制版本省心，于是喜新厌旧。\nBash\n安装之后，TERMINAL打开是CMD，这可不行，我一惯是用bash的。打开settings.json是这样子的：\n{\n    &quot;python.defaultInterpreterPath&quot;: &quot;E:\\\\software-data\\\\scoop\\\\apps\\\\miniconda3\\\\current\\\\python.exe&quot;,\n    &quot;launch&quot;: {\n        &quot;configurations&quot;: [\n          \n            \n \n        ]\n    },\n \n    &quot;Bash&quot;: {\n        &quot;source&quot;: &quot;Bash&quot;,\n        &quot;args&quot;: [],\n        &quot;icon&quot;: &quot;terminal-bash&quot;,\n        &quot;path&quot;: &quot;E:\\\\software-data\\\\scoop\\\\shims\\\\bash.exe&quot;\n      }\n    },\n    &quot;terminal.integrated.defaultProfile.windows&quot;: &quot;Bash&quot;,\n    &quot;launch&quot;: {\n      &quot;configurations&quot;: [\n      \n      ]\n    }\n}\n它里面有bash，不知道是不去去抄了我的VSCODE配置，但是配置又不对，有一个花括号对不上，于是终端就找不到Bash选项。参考一下我在VSCode的设定，改为以下内容：\n{\n    &quot;python.defaultInterpreterPath&quot;: &quot;E:\\\\software-data\\\\scoop\\\\apps\\\\miniconda3\\\\current\\\\python.exe&quot;,\n    &quot;terminal.integrated.env.windows&quot;: {},\n    &quot;terminal.integrated.profiles.windows&quot;: {\n    &quot;Bash&quot;: {\n        &quot;source&quot;: &quot;Bash&quot;,\n        &quot;args&quot;: [],\n        &quot;icon&quot;: &quot;terminal-bash&quot;,\n        &quot;path&quot;: &quot;E:\\\\software-data\\\\scoop\\\\shims\\\\bash.exe&quot;\n      }\n    },\n    &quot;terminal.integrated.defaultProfile.windows&quot;: &quot;Bash&quot;,\n    &quot;launch&quot;: {\n      &quot;configurations&quot;: [\n      \n      ]\n    }\n}\nBash就出来了。我不单在Windows里有bash，还配置得比较好看。\n\nR\n一打开，Positron能发现我有好多个python环境，但是R却找不到，主要是它在特定的安装路径和注册表以及RStudio的配置中去找，在我这通通找不到。\n我安装R稍微比较另类，请猛击《Windows下新装R的极简指南》，只要在配置中，把R的根目录给设定好就行，也还算比较简单。\n顺道吐槽，网上一堆代码，都告诉你，干啥先来一句rm(list=ls())，说是让你的R环境干净，本身一直在RStudio里不出来的所谓跑脚本，就是一个悖论。RStudio一打开，已经不干净了。你看我下面的截图，我一打开R，也已经不干净了。除了ASCII图和名人名言，一打开R已经加载了很多东西。我跑脚本，从来没有这一句rm(list=ls())，而是区分交互使用R和用R跑脚本两件事：一个是平时交互使用加一堆预设，让自己爽；一个是跑脚本追求干干净净，避免别人用脚本的时候，因环境不一致而出错，让别人爽。\n\n开箱稍微弄一下，Bash有了，R能用，Python能用，就OK了。相信专业人士的定制，会让我们用起来省心省力一些，其它的，其实和VSCODE没两样，因为就是一张皮。\n平时编辑文件习惯用code filename来打开，现在要用positron来打开，就得变成positron filename，为了照顾肌肉记忆，只需要在.bashrc里加入alias code=&quot;positron&quot;即可。"},"index":{"slug":"index","filePath":"index.md","title":"Guangchuang Yu's Blog","links":[],"tags":[],"content":"欢迎来到余光创的博客，主要记录本人在科研工作中的一些经验与思考、科研进展和学习笔记。\n课题组主页\n\n主页：yulab-smu.top/\nGitHub：github.com/YuLab-SMU\n\n课题组公众号\n欢迎关注课题组 公众号：YuLabSMU。"},"文章发表/文章发表：aplot让你轻松画出复杂的图":{"slug":"文章发表/文章发表：aplot让你轻松画出复杂的图","filePath":"文章发表/文章发表：aplot让你轻松画出复杂的图.md","title":"aplot让你轻松画出复杂的图","links":[],"tags":["aplot","paper","visualization"],"content":"aplot发表\naplot包发表在The Innovation期刊上。\n\n开发这个包的灵感来自于我在2018年发的MBE文章。\n\n可以说这是对数据整合和可视化长期思考所得，进行高度抽象才提出这两个方法，而这两个方法，分别激发了我开发出来aplot和ggtangle两个包。\n复杂图不再复杂\naplot作为一个拼图包，和大家所熟知的拼图包cowplot和patchwork是不一样的。不是把图放在一起，而是把图中的数据进行关联，就像文章图2所展示，你画几个图，无需知道数据间的关系，每个图画起来都比较简单，但是你一旦用aplot的insert_top、insert_bottom、insert_left和insert_right系列函数进行拼图，这些函数会试图去把数据给关联起来，对每个小图的内容进行重新排列，以使得图与图之间信息是对应的。最终出来底下G图这样复杂的图，如果要手搓这个图，难度很大，用aplot，很简单。\n\n这个包的重点，就是这个功能，是我开发它的初衷。期间还有点小插曲，写个包总有人想抄代码。借此分享这段tweet给大家。\n\n你也能创作复杂图\n能够拼的图种类是比较多的：\n\n当然你也可以自己创作一些，因为怎么拼随便你嘛。比如前面图2的单细胞marker基因表达热图，我就拼了通路富集分析结果。比如这里的A图中，oncoplot就被拿来再拼TCGA中相应基因在两个肿瘤中表达量分布。再比如E图，upsetplot我们可以用aplot拼出来，但既然是拼的，我干嘛不拼点别的呢？于是我们就搞了这个不一样的upsetplot。\n说到oncoplot，是由一个学生李申锁贡献的代码，详见《ggplot2版本的oncoplot来了》，在我面前露一手，然后跟我说要申请博士，但由于我没有名额，然后也就没有然后了。虽然接触时间短，也过了比较长一段时间，但贡献我是不会忘记的。\n\n普通的拼图\naplot另外的功能是plot_list()，就是普通的拼图的，图不会变，只是简单地组合起来放在一起。虽然patchwork包做得很好，但我是老狗学不了新把戏，早期用惯了cowplot::plot_grid，我就想来一个类似的，当然也不完全一样，我还是有我自己的一点东西在里面。\n\n啥都能拼：base plot、ggplot2、图片等\n\n\n\n\n万物皆可分面：这些拼在一起的图，只要你给个名字，就会变成像分面的label一样，在出图时呈现。\n\n\n保持YuLab团队的五毛效果：我们开发一些包，比如说ggbreak，它所呈现的效果，你用其它的拼图工具，可能就不兼容了，但我们自己，自成一统。配合得舒舒服服的。\n\n\nGA图创作\n\n最后讲讲这张GA图，一开始画了一版，呈现4个函数的功能，我觉得（1）和图一太重复；（2）这4个函数是可以自由组合，反复使用的，这点体现不出来。所以我提了意见，用一条彩带，后面虚化，代表未完待续，还能继续调用这些函数**。**然后可以把aplot拟人化，成为舞动彩带者。至于上面拼图的元素，我的想法是哆啦A梦的口袋，后面双斌提出来说感觉口袋是aplot这个人的，换成云。一开始设计师就在云上面平铺了几个图，我觉得不行，跟设计师说，想想天兵天将要捉拿孙悟空的场景，图要堆不同的层，后面可以虚化一些，体现人多势众的感觉。就这样出来我们这张GA图。\n我是第一人\n\naplot的灵感来自于2018年我的文章，在ggtree中孕育了好几年。然后我在2020年就开发出来并放出到CRAN上。解决我前面小标题写的复杂图不再复杂这个问题，aplot必须是第一个工具。现在有类似的工具，包括python的Marsilea和R的ggalign。第一个是最难的，因为你要产生想法，要在没有任何借鉴的情况下去实现。现在aplot的功能可能不是最全的，但aplot肯定是最容易用的，能解决你大多数的问题。"},"文章发表/文章发表：小丫画图，代码开放，注释中英文对照":{"slug":"文章发表/文章发表：小丫画图，代码开放，注释中英文对照","filePath":"文章发表/文章发表：小丫画图，代码开放，注释中英文对照.md","title":"小丫画图，代码开放，注释中英文对照","links":[],"tags":["FigureYa","paper","visualization"],"content":"\n作为_iMetaMed第一卷第一期的封面文章发表，也就是说创刊号的封面文章，说起来好像蛮有纪念意义一样。特别是如果iMetaMed_能够成为一个高攀不起的期刊的话。\n\n文章的解读可以参考：《iMetaMed | 大连医科大学葛瑛-FigureYa：一个用于提升生物医学数据解读与研究效率的标准化可视化框架》，还有从小丫个人角度的版本：《FigureYa更新啦！借助学术的力量》。\n\n这个评论谬赞了，但对于小丫画图来说，真的就是这个味。当年我博士还没毕业的时候，小丫说想要搞个众筹模式，来帮助大家实现下游的一些数据可视化需求，问我能不能来支持一下。我大概说我可以先来打个样，记得小丫说有我的支持，她有信心可以搞起来。印象中大概我还有写过一句lapply()出几个boxplot，然后对接cowplot::plot_grid，直接出一张图的代码。然后果子大加称赞，觉得不看我代码，只会for循环。近十年了，应该没记错，还没老年痴呆。\n\n今时今日的你可能无法想象画一个箱线图都会有困难，但事实就是小丫画图帮助了无数的研究生，后来各种各样的画图云平台，我们其实也可以看到不少小丫画图的身影。至于说“AI时代了，画图有何难，AI帮你写代码”，那AI的知识库也是无数的文档、教程、博客、论坛问题等等，是无数具有互联网分享精神的网友们打下的基础，让我们用上面这一页PPT致敬一下那个时代。\n后面我基本不去写，但有介绍学生去接活，也有给一些人指导，属于偶尔幕后一下。其中，有一个代码，我指导他打成R包，后面还拿去发了一篇小文章，现在也是一篇ESI高被引论文。\n\n这个事情应该说，帮助了很多学生，想要实现的图搞不出来，提出需求，众筹成功了，就能拿到带有示例数据、中文注释、能跑通的代码。如果要画的图，是现有的，就直接付费。一杯奶茶的费用，可以说毫无负担。随着时间的推移，积累的图越来越多，有317个。毫无疑问，对广大研究生的课题推进，是有帮助的。\n其实我想强调的是，花一杯奶茶的钱，不单单是要把图画出来，这是赶工着急的时候干的事，只想要结果。我们还应该在有闲的时候，回过头来看这个代码。这是一个学习的过程，这杯奶茶钱就花得太值。当然现在文章发表了，代码开放了，还重新整理，加了中英文注释。不单单是免费的工具，再次强调一下，也是一个很好的学习资源。不知道为什么，这年代叫人学习，天打雷劈。"},"机器学习与统计方法/AI学起来，毕竟AI让地球运转":{"slug":"机器学习与统计方法/AI学起来，毕竟AI让地球运转","filePath":"机器学习与统计方法/AI学起来，毕竟AI让地球运转.md","title":"AI学起来，毕竟AI让地球运转","links":[],"tags":["DeepLearning"],"content":"Andrew Ng在搞Coursera之前，有个OpenClass，就是教Machine Learning，那是远在2010年，非常久远，但是可以说，后面的那些课程，都是在那个基础上弄的。其实差不多，另一方面就是那个年代久远，Python根本不像现在这样流行，那个年代还整天在Perl VS Python，而不是今天的R VS Python。然后这个OpenClass的课程，是用Matlab/Octave教的，我印象中应该是Matlab，然后课程作业我是用Octave作的。然后我又自己用R重新写了，打了一个R包，取名mlass是ML + clAss的意思，无意中变成了ML + ass。\n\n我大概实现了大部分基础的东西，神经网络没有写。\n\nLinear Regression with one variable &lt;2012-01-06, Fri&gt;\nLinear Regression with multiple variables &lt;2012-06-28, Thu&gt;\nLogistic Regression &lt;2012-06-20, Wed&gt;\nRegularization for preventing overfitting &lt;2012-06-27, Wed&gt;\nNeural Networks\nSupport Vector Machines &lt;2012-04-11, Wed&gt;\nHierarchical clustering, single and complete method &lt;2012-12-10, Mon&gt;\nHierarchical clustering, average (UPGMA) method &lt;2014-08-22, Fri&gt;\nK-Means algorithm &lt;2011-12-30, Fri&gt;\nPrincipal Component Analysis\n\n但是这个十几年前写的包，还是挺好的，因为除了实现算法之外，结果用S4对象存，写了方法访问数据，写了可视化代码画图，然后部分计算的东西，我还用C++写了。当年的session info，ggplot2还是0.9.0，非常古早。\n\n所谓人丑要多读书，AI得继续学起来，于是翻出来这个旧代码，发现这个包安装报错了，修正了一下。文档也从原来用LaTeX写的，换成了用Markdown来写。它又可以安装了。\n例子大概比如kmeans里迭代中心点，画出来这样子：\n\n再比如一个SVM分类的例子，结果是这样子：\n\n前两年大家还在说大数据就像少年性事，大家都在讲但没人做过。现在AI都快成为大学生的通识课了。必须要搞起来。"},"机器学习与统计方法/KMeans应用于非线性数据":{"slug":"机器学习与统计方法/KMeans应用于非线性数据","filePath":"机器学习与统计方法/KMeans应用于非线性数据.md","title":"KMeans应用于非线性数据","links":[],"tags":["MachineLearning","KMeans","Classification"],"content":"书接上一回，对于线性不可分的数据，怎么用KMeans来做聚类。\n数据\n用make_moons来生成一个比较复杂的流形数据。\nfrom sklearn.datasets import make_moons\n \nX, y = make_moons(200, noise=0.05, random_state=0)\n画图看一下，数据是长这样子的：\nimport matplotlib.pyplot as plt\n \nplt.scatter(X[:, 0], X[:, 1], s=50, c=y, cmap=&#039;viridis&#039;)\n\n简单的KMeans是分不好的\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2)\nlabels = kmeans.fit_predict(X)\nplt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap=&#039;viridis&#039;)\n我们用KMeans来聚类，然后用聚类结果进行上色：\n\n效果只能说不行。\nt-SNE + KMeans\n做过单细胞分析的，都知道t-SNE，当然你也可以试试UMAP，效果应该差不多。t-SNE擅长捕捉局部结构，就特别适合这种流形数据。它在高维空间用高斯分布计算样本间的相似度，然后用t分布匹配高维概率分布，通过梯度下降优化低维嵌入，优化目标是最小化高维和低维概率分布的KL散度。\n我们对数据先进行标准化：\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n再进行t-SNE的变换：\nfrom sklearn.manifold import TSNE\n \n \nX_tsne = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X_scaled)\ny_tsne_kmeans = KMeans(n_clusters=2).fit_predict(X_tsne)\n这时候，我们可以对变换后的数据进行可视化：\nplt.scatter(X_tsne[:,0], X_tsne[:,1], c=y, cmap=&#039;viridis&#039;, s=50)\n\n非常好地保留了原来的分类信息，然后这个数据就成了线性可分了。\n那么我们可以可视化原始的数据，并用KMeans聚类结果进行上色，看看聚类效果如何：\nplt.scatter(X[:, 0], X[:, 1], c=y_tsne_kmeans, s=50, cmap=&#039;viridis&#039;)\n\n谱聚类\n谱聚类本质上就是核KMeans的优化版本，就是上次说的应用核函数再加KMeans，当然如果你自己这么简单做了，效果可能不太好，因为谱聚类做得更多，所以更加鲁棒。它先计算一个相似度矩阵（可以通过高斯核函数+欧式距离，也可以通过KNN连接矩阵），再构建拉普拉斯矩阵对相似度矩阵进行归一化，再计算特征向量（本质就是低维嵌入），然后对特征向量运行KMeans。\nfrom sklearn.cluster import KMeans, SpectralClustering\nspectral = SpectralClustering(n_clusters=2, \n                             affinity=&#039;rbf&#039;,\n                             gamma=15,\n                             random_state=42)\ny_spectral = spectral.fit_predict(X_scaled)\nplt.scatter(X[:, 0], X[:, 1], c=y_spectral, s=50, cmap=&#039;viridis&#039;)\n这里我就用了affinity=&#039;rbf&#039;也就是高斯核函数，gamma这个参数控制高斯核函数的宽度，数据复杂的情况下，gamma就可以调大一点试试。最后的聚类结果效果就非常好。\n"},"机器学习与统计方法/深度学习/PyTorch对数据进行多分类":{"slug":"机器学习与统计方法/深度学习/PyTorch对数据进行多分类","filePath":"机器学习与统计方法/深度学习/PyTorch对数据进行多分类.md","title":"PyTorch对数据进行多分类","links":[],"tags":["DeepLearning","Pytorch","Classification"],"content":"书接上一回，从二分类往前走两步，这次来做多分类。\n数据\nimport numpy as np\nimport matplotlib.pyplot as plt\nRANDOM_SEED = 42\n \nnp.random.seed(RANDOM_SEED)\n \nN = 100\nD = 2\nK = 3\n \nX = np.zeros((N*K, D))\ny = np.zeros(N*K, dtype=&#039;uint8&#039;)\n \nfor j in range(K):\n    ix = range(N*j, N*(j+1))\n    r = np.linspace(0.0, 1, N)\n    t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N) * 0.2\n    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n    y[ix] = j\n生成个三分类的数据来训练模型，对于模型来说，多少个分类都是一样训的。\n数据大概是长这样子的：\n\n数据现在是numpy，要转成tensor，然后要放到合适的device（放到GPU去），然后还要切分数据为训练集和测试集。\nimport torch\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.LongTensor)\n \nX = X.to(device)\ny = y.to(device)\n \nfrom sklearn.model_selection import train_test_split\n \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n模型\nfrom torch import nn\n \nClassification = nn.Sequential(\n    nn.Linear(in_features = 2, out_features = 10),\n    nn.ReLU(),\n    nn.Linear(in_features = 10, out_features = 10),\n    nn.ReLU(),\n    nn.Linear(in_features = 10, out_features = 3)\n).to(device)\n这和我们上次做二分类有什么区别，可以说根本没区别，当然如果你仔细看的话，原来二分类的输出是一个logit，然后我们过一个sigmoid，出来分类的概率。这里是多分类，对每一个分类都会有一个logit，所以输出的长度与分类的数目一致，这里是3分类，就设定out_features = 3。这长度为3的logits，依然需要有一个函数给转成这三种分类的概率，这就需要用softmax函数。预测的结果就是概率最大的分类。\n\n损失函数\nloss_fn = nn.CrossEntropyLoss()\n正如我们在二分类里介绍的，分类用交叉熵来计算损失，这里相应地就用了nn.CrossEntropyLoss()函数。\n优化器\noptimizer = torch.optim.Adam(params = Classification.parameters(), lr = 0.01)\n\n上次我们用随机梯度下降（SGD），这次我们换一个，用Adam，Adam这个方法的基本思路是融合了两种方法：Momentum和AdaGrad。Momentum参照小球在碗中滚动的物理规则进行移动；AdaGrad会记录过去所有梯度的平方和，学习越深入，更新的幅度就越小。Adam通过组合这两个方法的优点，有望实现参数空间的高效搜索。\n准确率\nfrom torchmetrics import Accuracy\n \nacc_fn = Accuracy(task=&quot;multiclass&quot;, num_classes = K).to(device)\nacc_fn\n准确率这个函数和我们做二分类基本一样，差别就在于指定了不同的num_classes参数而已。\n训练模型\nfrom timeit import default_timer as timer \n \nstart_time = timer()\n \nepochs = 1000\n \nfor epoch in range(epochs):\n    ## Training\n    Classification.train()\n \n    y_logits = Classification(X_train)\n    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n \n    loss = loss_fn(y_logits, y_train)\n    acc = acc_fn(y_pred, y_train)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n \n    ## Testing\n    Classification.eval()\n    with torch.inference_mode():\n        test_logits = Classification(X_test)\n        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n        test_loss = loss_fn(test_logits, y_test)\n        test_acc = acc_fn(test_pred, y_test)\n \n    if epoch % 100 == 0:\n        print(f&quot;Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}&quot;)\n \nend_time = timer()\n和二分类的训练代码基本上是一样的，就是把模型一换，把logits要过一下sigmoid换成softmax而已。\n\n也是很快就收敛了，准确率很高，用时5秒不到。\n\n可视化\n最后我们把模型在训练集和测试集的决策边界给画一下：\n"},"机器学习与统计方法/深度学习/卷积神经网络CNN":{"slug":"机器学习与统计方法/深度学习/卷积神经网络CNN","filePath":"机器学习与统计方法/深度学习/卷积神经网络CNN.md","title":"卷积神经网络CNN","links":[],"tags":["DeepLearning","CNN","Pytorch","Classification"],"content":"数字识别用传统的机器学习就可以做到蛮高的准确率，使用神经网络，完全也可以用全连接的网络来识别，无非是一个多分类的问题。和我们平时做多分类没有差别。这里我们用卷积神经网络（CNN）来做，CNN在图像识别上应用非常广泛。因为全连接神经网络的输入是一个向量，把图片按照行连接成一个向量，就会把空间信息给抹掉。这就是CNN要解决的，要利用空间信息，来找patterns，比如要识别一只鸟，图片中有一小块是一个鸟嘴，这对识别鸟是有用的。\nCNN概述\nCNN和全连接的神经网络的区别首先在在于它传递的数据是有形状的，也就是前面说的要利用空间信息，所以用sliding windows的策略，扫描一个又一个的窗口（patches），这个窗口每次扫描就用一个滤波器（filter）去做inner product，一个窗口出来是一个值。\n\n这一个滤波器作用于这一个通道上，会出来另一个大小不一样的二维数据，所以输入输出有时候又可以统一称之为特征图。输入通常不是一个通道，比如说RGB的图，就有RGB三个通道，最终的输出是每一个通道都一样的做法，然后将多个通道进行加和。\n\n滤波器主要用于提取图像的特征，比如下面这张图所演示的。\n\n一个滤波器出来一张特征图，多个滤波器就会出来多个特征图，就像RGB的图有三个通道一样，用N个滤波器，出来的数据就有N个通道，每个通道是2D的数据，整个是一个3D的数据，我们依然可以理解成一张图，原来的图是3个通道，现在有N个通道了，不是我们理解的RGB图像，但形式上是一致的。这就是卷积的过程。\n\n滤波器里的数值，相当于我们在全连接神经网络中权重，也会加上偏置，这些参数都是学习出来的。\n\n一个典型的卷积神经网络就是卷积→激活→池化，可以是一个这样的过程，也可以连着搭几个。这个做为Feature learning，把图像的特征学出来，然后用这些特征做为输入，接一个全连接的神经网络（也被称之为dense network），这就是完整的CNN。\n卷积层\n\n运算就是前面讲到的相当于图像处理的滤波器运算。这里涉及到一些和运算细则有关的概念，包括kernel size, padding和stride，这些会影响到输出的数据形状大小，这也要算一下的，因为最终要过一下全连接层，输入的大小是要准确指定的。\n滤波器（filter/kernel）\n滤波器有时候也叫kernel，因为滤波器是不需要我们指定的，也是学出来的，我们要指定的只有滤波器的数量，也是输出的channel数目，还有滤波器的大小，kernel size。\n填充（padding）\n一个4x4的数据，用一个3x3的滤波器，输出的数据就变成了2x2，所以这样子，如果叠加卷积层，就有可能在中间变成1x1，后面没法再继续做卷积了。所以就需要在数据的边缘进行填充，像下面这个例子，通过填充就可以让输入和输出保持4x4的形状。\n\n步幅（stride）\n在应用滤波器时，通过移动窗口，把所有的数据都过一遍，窗口和窗口之间是有重叠的，移动窗口的步伐有多大，就是步幅。\n\n池化\n典型的CNN包括卷积→激活→池化，激活就是ReLU这些，都很清楚，最后就池化：\n\n池化没有需要学习的参数，就是在目标区域里取个代表性的值，上图是最大值，也可以是平均值。池化是在每个通道单独进行，所以不会改变通道数。\n输入数据发生一点微小的变化，通过池化，输出基本上是不变的，这样就具有一定的鲁棒性。当然其实池化最主要的作用是让数据变小，这本身是为了更高效的计算。从这个角度来说，计算资源够的情况下，是可以不需要池化的。\n书上讲的都是说让数据变小这一点，我自己想到的是，因为卷积运算可以套好几个，那么经过池化之后，再做同样大小的滤波运算，相当于在更大的区域去捕获特征了。我认为是有这个作用的，相当于前面是低空看细节，后面高空看轮廓，有这样一个过程的作用。\n另外也要看实际情况，对于一般的图像处理来说，池化的过程，相当于压缩了图片，压缩完图片看着变化不大。AlphaGO就使用了CNN，但是它就没有用池化，因为池化了之后，棋盘就不完整了，这和图像处理是有区别的，所以它不能够用池化。所以说，还得具体情况具体分析。\nPyTorch实战\n数据\n这里使用的是MNIST手写数字的数据集，有6万张图片。先创建文件夹，然后下载，如果文件已经存在，就跳过下载。\nfrom pathlib import Path\nimport requests\n \nDATA_PATH = Path(&quot;data&quot;)\nPATH = DATA_PATH / &quot;mnist&quot;\n \nPATH.mkdir(parents=True, exist_ok=True)\n \nURL = &quot;github.com/pytorch/tutorials/raw/main/_static/&quot;\nFILENAME = &quot;mnist.pkl.gz&quot;\n \nif not (PATH / FILENAME).exists():\n    content = requests.get(URL + FILENAME).content\n    (PATH / FILENAME).open(&quot;wb&quot;).write(content)\n这个数据是个pickle文件，存的是numpy array格式，我们相应地把它解压缩，讲进来。\nimport pickle\nimport gzip\n \nwith gzip.open((PATH / FILENAME).as_posix(), &quot;rb&quot;) as f:\n    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=&quot;latin-1&quot;)\n总共有6万张图片，5万张是训练集，1万张做为验证/测试集。\n\n图片是28x28像素的，已经被打成长度为784的一维向量。\n我们可以随便选一张画出来看看：\nfrom matplotlib import pyplot as plt\nimport numpy as np\nplt.imshow(x_train[9].reshape((28,28)), cmap=&#039;gray&#039;)\n\nNumPy to Tensor\n因为是numpy array格式，我们需要转换成tensor。\nimport torch\n \nx_train, y_train, x_valid, y_valid = map(\n    torch.tensor, (x_train, y_train, x_valid, y_valid)\n)\nDataLoader\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\n \nbs = 64\n \ntrain_ds = TensorDataset(x_train, y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs)\n \nvalid_ds = TensorDataset(x_valid, y_valid)\nvalid_dl = DataLoader(valid_ds, batch_size=bs * 2)\n我们使用TensorDataSet来存这些数据，同时存samples和labels，再用DataLoader来方便我们访问和迭代这些数据，其中一个参数是batch_size，也就是批处理的大小，指定一次有多少个数据打包喂给神经网络。\n模型\n首先是看看有没有GPU，有则设备设为’cuda’，没有就设为’cpu’。我们的运算都在指定的device上进行。\ndevice = torch.device(&#039;cuda&#039; if torch.cuda.is_available() else &#039;cpu&#039;)\n训练步骤\n训练的代码都基本一样，我们把它写成一个函数，方便调用。\ndef train_step(model: torch.nn.Module,\n               data_loader,\n               loss_fn,\n               optimizer,\n               acc_fn,\n               device = device):\n \n    train_loss, train_acc = 0, 0\n    for batch, (X, y) in enumerate(data_loader):\n        X, y = X.to(device), y.to(device)\n        model.train()\n        y_pred = model(X)\n \n        loss = loss_fn(y_pred, y)\n        train_loss += loss\n        train_acc += acc_fn(y_pred, y)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n \n    train_loss /= len(data_loader)\n    train_acc /= len(data_loader)\n \n    print(f&quot;Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}&quot;)\n测试步骤\n测试步骤也是同样的道理，也写成函数：\ndef test_step(model,\n              data_loader,\n              loss_fn,\n              acc_fn,\n              device = device):\n \n    test_loss, test_acc=0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            X, y = X.to(device), y.to(device)\n            \n            test_pred = model(X)\n \n            test_loss += loss_fn(test_pred, y)\n            test_acc += acc_fn(test_pred.argmax(dim=1), y)\n            \n        test_loss /= len(data_loader)\n        test_acc /= len(data_loader)\n \n        print(f&quot;Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}\\n&quot;)\nCNN模型\n我们用poloclub.github.io/cnn-explainer/里的CNN架构，包含两个结构一样的blocks。\n\n这个架构是有名的VGG的简化版本，TinyVGG:\n\nfrom torch import nn\n \nclass MNIST(nn.Module):\n    def __init__(self, input_shape, hidden_units, output_shape):\n        super().__init__()\n        self.block_1 = nn.Sequential(\n            nn.Unflatten(1, (1, 28, 28)),\n            nn.Conv2d(\n                in_channels = input_shape,\n                out_channels = hidden_units,\n                kernel_size = 3,\n                stride=1,\n                padding=1),\n            nn.ReLU(),\n            nn.Conv2d(\n                hidden_units, hidden_units, 3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2)\n        )\n \n        self.block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n \n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(in_features=hidden_units*7*7,\n                      out_features=output_shape)\n        )\n \n    def forward(self, x):\n        return self.classifier(self.block_2(self.block_1(x)))\n两个blocks都是Conv2d→ReLU→Conv2d→ReLU→MaxPool2d。参数前面都有解析过相应的概念了，这里需要讲的是在block_1里用了nn.Unflatten，是因为原来的数据28x28像素已经被转成一维向量了，所以此处需要搞回2维的数据。然后这里的卷积运算，用的kernel_size=3，stride=1，padding=1，这样子数据的形状大小是不变的；再通过MaxPool2d的时候，kernel_size=2, stride=2，所以一个2x2的数据就变成了1个数，数据就从28x28，变成了14x14。有两个blocks，过了两次MaxPooling，最终就变成了7x7，所以最后给nn.Linear的数据，就是7x7xhidden_units（也就是最后数据的通道数）。\n训练模型\n先初始化一个模型，input_shape就是传给卷积层的通道数，这里是灰度图，所以只有一个通道，hidden_units就是中间的通道数，而output_shape就是最后过全连接神经网络的输出，因为是0-9的数据，所以是10个分类。\ntorch.manual_seed(42)\n \nmodel = MNIST(input_shape=1, hidden_units=10, output_shape=10).to(device)\n\n还有相应的损失函数和优化器等：\nfrom torchmetrics.classification import Accuracy\nacc_fn = Accuracy(task=&quot;multiclass&quot;, num_classes=10).to(device)\n \nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model.parameters(), \n                             lr=0.1)\n万事俱备，我们就可以开始训练了，有了前面训练步骤和测试步骤两个函数，就是把这些模型、损失函数、准确率函数和优化器给传进去就可以了。换一个模型和相应的这些函数，再传进去，就变成了训练另一个模型。\nfrom timeit import default_timer as timer \n \nstart_time = timer()\n \nepochs = 3\nfor epoch in range(epochs):\n    print(f&quot;Epoch: {epoch}\\n---&quot;)\n    \n    train_step(model=model, \n               data_loader=train_dl, \n               loss_fn=loss_fn, \n               optimizer=optimizer,\n               acc_fn=acc_fn)\n \n    test_step(model=model, \n              data_loader=valid_dl,\n              loss_fn=loss_fn,\n              acc_fn=acc_fn)\n \nend_time = timer()\n\n准确率达到了98%，6万张图片过了3遍，只用了17秒。\n评估\ny_preds = []\nmodel.eval()\nwith torch.inference_mode():\n  for X, y in valid_dl:\n    X, y = X.to(device), y.to(device)\n    y_logit = model(X)\n    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)\n    y_preds.append(y_pred.cpu())\n    \ny_pred_tensor = torch.cat(y_preds)\n把验证数据集传给模型，把预测的结果拿到。我们就可以用前面定义的acc_fn来算一下准确率。\n\n和前面那个测试步骤最后的准确率是一样的，因为我们这里用的同一个数据集，也就是说这个数据集，既拿来测试，又拿来验证。一般情况下，我们可以把数据分成3个数据集，一个训练，一个验证，一个测试。因为这个下载的MNIST数据，本身就是分两块的，懒得再去切分它，就先这样用了。\n单纯一个准确率，是不清楚那些分类做得不好的，我们可以用混淆矩阵来看一下。\nfrom torchmetrics import ConfusionMatrix\nconfmat = ConfusionMatrix(num_classes=10, task=&#039;multiclass&#039;)\nconfmat_tensor = confmat(preds=y_pred_tensor, target=y_valid)\n这个代码就计算出了混淆矩阵，我们再画个图来看：\nfrom mlxtend.plotting import plot_confusion_matrix\n \nfig, ax = plot_confusion_matrix(\n    conf_mat = confmat_tensor.numpy(),\n    class_names = range(10)\n)\n"},"机器学习与统计方法/深度学习/用PyTorch训练简单线性回归":{"slug":"机器学习与统计方法/深度学习/用PyTorch训练简单线性回归","filePath":"机器学习与统计方法/深度学习/用PyTorch训练简单线性回归.md","title":"用PyTorch训练简单线性回归","links":[],"tags":["DeepLearning","Pytorch","Regression"],"content":"上次手搓一个简单的线性回归，这次该上PyTorch了。\nimport torch  \nfrom torch import nn\n数据\n还用上一次的数据：\n\n先转成Tensor:\nX = torch.tensor(df[&#039;population&#039;].values, dtype=torch.float)  \ny = torch.tensor(df[&#039;profit&#039;].values, dtype=torch.float)  \nX = X.view(len(X), 1)  \ny = y.view(len(y), 1)\n定义模型\nclass LinearRegressionModel(nn.Module):  \n    def __init__(self):  \n        super().__init__()  \n        self.linear_layer = nn.Linear(in_features=1, out_features=1)  \n      \n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:  \n        return self.linear_layer(x)\n然后我们需要初始化一个模型对象，指定用什么loss function和optimizer：\nmodel = LinearRegressionModel()  \n  \nloss_fn = nn.L1Loss()  \n  \noptimizer = torch.optim.SGD(params=model.parameters(), lr=0.01)\n训练\n无非是搭积木：\nepochs = 5000  \n  \nfor epoch in range(epochs):  \n    model.train()  \n      \n    y_pred = model(X)  \n    loss = loss_fn(y_pred, y)  \n      \n    optimizer.zero_grad()  \n    loss.backward()  \n    optimizer.step()\n结果\n"},"机器学习与统计方法/深度学习/神经网络对数据进行二分类":{"slug":"机器学习与统计方法/深度学习/神经网络对数据进行二分类","filePath":"机器学习与统计方法/深度学习/神经网络对数据进行二分类.md","title":"神经网络对数据进行二分类","links":[],"tags":["DeepLearning","Classification","Pytorch"],"content":"\n没有GPU，无法AI :)\n数据\n这个数据是随机生成的，用的sklearn里的make_moons\nRANDOM_SEED = 42\n \nfrom sklearn.datasets import make_moons\n \nn_samples = 1000\n \nX, y = make_moons(n_samples = n_samples, noise = 0.2, random_state=RANDOM_SEED)\n画个图看一眼：\nimport pandas as pd\n \nX_df = pd.DataFrame(X)\n \nimport matplotlib.pyplot as plt\n \nplt.scatter(X_df[0], X_df[1], c=y)\n\n因为我们训练是在GPU上，所以数据我们需要也放到GPU上，再者数据要拆分为训练集和测试集，这里依然使用sklearn中的函数，train_test_split，80%的数据用于训练，20%的数据用于测试。\nX = torch.tensor(X, dtype=torch.float).to(device)\ny = torch.tensor(y, dtype=torch.float).to(device)\n \nfrom sklearn.model_selection import train_test_split\n \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n模型\n继续我们的PyTorch学习，上一次在《用PyTorch训练简单线性回归》中我们定义了继承nn.Module的类。对于简单的神经网络来说，我们用nn.Sequential就可以了，连forward函数都省了。\nimport torch\nfrom torch import nn\n \nBinaryClassification = nn.Sequential(\n    nn.Linear(in_features = 2, out_features = 10),\n    nn.ReLU(),\n    nn.Linear(in_features = 10, out_features = 10),\n    nn.ReLU(),\n    nn.Linear(in_features = 10, out_features = 1)\n).to(device)\n\n这里用的数据是个二维的数据，当然多少维都一样，就是in_features数改一下而已，这里二维，进来就接10个神经元，我们也可以认为是对数据进行了升维，然后再接10个神经元，最后的输出就是所谓的logit，所以分类叫logistic regression嘛。把这个logit值再过一个sigmoid函数，就是分类的概率了。这里每一层线性回归，都要加一个激活函数，这里用了ReLU，但也可以是别的。这就是我在《简单线性回归》最后写的：\n\n损失函数\nloss_fn = nn.BCEWithLogitsLoss()\n这里是分类，分类的损失函数可以用交叉熵（Cross Entropy），这里是二分类就有相应的Binary Cross Entropy，所谓损失函数，就是比较真实label和预测的label的差距，优化的过程就是希望这个差距越来越小。我们的模型输出的是logits，需要过一个sigmoid，才能预测label，然后才能使用交叉熵来计算损失，所以是两步。那么这里使用的nn.BCEWithLogitsLoss()就是整合了这两步，而且会比自己拆分两步来计算在数值上会更稳健一些。\n优化器\noptimizer = torch.optim.SGD(params = BinaryClassification.parameters(), lr = 0.1)\n这里用了torch.optim.SGD也就是随机梯度下降，这是很常见的优化器，这里设置学习率为0.1。\n准确率\n我们需要定义一个函数，来度量一下模型的准确率，这个函数很简单，就是预测的label和真实的label是否相等的平均值（相等的个数/总数）。虽然函数很简单，但我们不需要自己定义，可以用torchmetrics，用这个模块还能方便我们在需要的时候用更多的指标。\nfrom torchmetrics.classification import Accuracy\n \nacc_fn = Accuracy(task=&quot;multiclass&quot;, num_classes=2).to(device)\n训练模型\nfrom timeit import default_timer as timer \n \nstart_time = timer()\n \nepochs = 1000\n \nfor epoch in range(epochs):\n    ## Training\n    BinaryClassification.train()\n \n    y_logits = BinaryClassification(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits))\n \n    loss = loss_fn(y_logits, y_train)\n    acc = acc_fn(y_pred, y_train)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n \n    ## Testing\n    BinaryClassification.eval()\n    with torch.inference_mode():\n        test_logits = BinaryClassification(X_test).squeeze()\n        test_pred = torch.round(torch.sigmoid(test_logits))\n        test_loss = loss_fn(test_logits, y_test)\n        test_acc = acc_fn(test_pred, y_test)\n \n    if epoch % 100 == 0:\n        print(f&quot;Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%&quot;)\n \nend_time = timer()\n这里使用了timeit来记录时间，epochs用了1000，其实不用这么多，很快就收敛了。每个epoch里分两段，一段是训练，就是数据进去，计算损失，优化参数。另一段是是用测试集进行测试，这里用了torch.inference_mode()，因为测试的时候，没有优化过程，也就不需要去跟踪参数，会更快更省内存，测试阶段就只需要把数据扔进模型，计算一个损失和准确率。\n最后是每100个epochs，打印一下相应的信息，让我们了解模型训练的表现。\n\n到了100个epochs的时候，准确率在训练集和测试集上都已经&gt;80%了\n\n所用的时间，不到3秒。\n可视化\n最后我们可以画一下，模型在训练集和测试集中的分类情况：\n"},"机器学习与统计方法/深度学习/简单线性回归":{"slug":"机器学习与统计方法/深度学习/简单线性回归","filePath":"机器学习与统计方法/深度学习/简单线性回归.md","title":"简单线性回归","links":[],"tags":["DeepLearning","Regression"],"content":"上一次说AI学起来，这就学起来。\n先来个简单的线性回归。\n假如要开一家餐馆，街区人口和利润是关联，从已经开的店的数据，预测一下，预测在新的街区开店的收入是多少。\n读入数据\nimport pandas as pd  \ndf = pd.read_csv(&quot;data/street-profits.csv&quot;, names=[&#039;population&#039;, &#039;profit&#039;])\ndf.head()  \n  \npopulation profit  \n0 6.1101 17.5920  \n1 5.5277 9.1302  \n2 8.5186 13.6620  \n3 7.0032 11.8540  \n4 5.8598 6.8233\n线性回归\ny = theta_0 + theta_1 * x，把 x加一个column是1，这样子就可以变成y = theta @ X.\nimport numpy as np  \ndf[&#039;b&#039;] = np.ones(df.shape[0])  \nX = df[[&#039;population&#039;, &#039;b&#039;]]  \ntheta = np.zeros(X.shape[1])  \ny = df.profit\n用均方误差（MSE）来定义cost function。\ndef mse(theta, X, y):  \n    # MSE: Mean Squared Error  \n    m = X.size  \n    inner = X @ theta - y  \n    square_sum = inner.T @ inner  \n    cost = square_sum / (2*m)  \n    return cost\n线性回归变成一个优化的问题，就是让代价函数最小。进行偏微分，就可以得到参数更新的方向和大小。alpha是所谓的learning rate。\n:= \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\bigl( h_\\theta(x^{(i)}) - y^{(i)} \\bigr) x_j^{(i)}$$\n这个公式其实是我们大一都学过的泰勒公式。  \n\n![](assets/简单线性回归/file-20251219233631986.png)\n\n在x接近a的时候，(x-a)就很少，后面多次方的项就可以忽略，这就成为了上面的公式了，而这个(x-a)就是alpha，就是在这里被称为learning rate的这一项，也就是说这个公式本身要成立，前提就是alpha很小。  \n\n下面的`gradient`函数算的就是偏微分的这块：\n\n```python\ndef gradient(theta, X, y):  \n    m = X.size  \n    inner = X.T @ (X @ theta - y)  \n    return inner/m\n```\n### 批量梯度下降算法\n\n按照上面的公式来计算。\n\n```python\ndef batch_gradient_descent(theta, X, y, epoch, alpha=0.01):  \n    cost_data = [mse(theta, X, y)]  \n    _theta = theta.copy()  \n  \n    for _ in range(epoch):  \n        _theta = _theta - alpha * gradient(_theta, X, y)  \n        cost_data.append(mse(_theta, X, y))  \n    return _theta, cost_data\n```\n\n搞个5000次迭代，出来最后的参数：\n```python\nepoch = 5000  \nfinal_theta, cost_data = batch_gradient_descent(theta, X, y, epoch)\n```\n\n\n\n## scikit-learn\n\n比较一下，才知道行不行。\n\n```python\nfrom sklearn import linear_model   model = linear_model.LinearRegression()   model.fit(X, y)`\n```\n\n![Image](mmbiz.qpic.cn/mmbiz_png/MPBFtnFrw4kcgwEwBWf2ibGdkNBMW5waHlAfnfOqLhWgLSibCK0IWJicnD2Sxc6bEcDZeoc9UuQCmf5z611icV79oA/640#imgIndex=1)\n\n学会了线性回归，就离神经网络不远了😄\n\n\n因为每个神经元都是一个线性回归，再加一个激活函数。神经网络就是一堆神经元，而再多的线性回归组合在一起，它还是一个线性函数，以二维来说，就还是一条直线，那有没有办法拟合任意曲线，那就需要带拐弯的，这就是激活函数要干的事，简单如ReLU，就是带个拐，就可以了。有拐弯，只要够多，经过缩放、位移、组合，就可以各种拐，而只要点足够多，直线拐也能平滑地拟和曲线，所以它就能拟合所有曲线，推广到高维空间，也是一样。"},"机器学习与统计方法/简单的KMeans":{"slug":"机器学习与统计方法/简单的KMeans","filePath":"机器学习与统计方法/简单的KMeans.md","title":"简单的KMeans","links":[],"tags":["MachineLearning","Classification","KMeans"],"content":"KMeans算法有两个假设：\n\n\n“簇中心点”是属于该簇的所有数据点坐标的算术平均值。\n\n\n一个簇的每个点到该簇中心点的距离，比到其他簇中心点的距离短。\n\n\nsklearn实例\n数据\nfrom sklearn.datasets import make_blobs\n \nX, y = make_blobs(n_samples = 300, centers = 4, cluster_std=0.5, random_state=0)\n用sklearn生成一个示例数据。我们画个图看看：\nimport matplotlib.pyplot as plt\n \nplt.scatter(X[:, 0], X[:, 1], s=50)\n\n很清楚，就是4个类。然后我们用kmeans算法来聚类。\nKMeans聚类\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(X)\n \ny_pred = kmeans.predict(X)\nsklearn的套路：1. 初始化模型（包括指定一些超参数），2. 拟合数据（或者叫训练），3. 预测。打完收工，我们画个图看看：\nplt.scatter(X[:, 0], X[:, 1], s=50, c=y_pred, cmap=&#039;viridis&#039;)\n\n我们也可以把中心点画在上面：\nplt.scatter(X[:, 0], X[:, 1], s=50, c=y_pred, cmap=&#039;viridis&#039;)\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c=&#039;red&#039;, s=200, alpha=0.8)\n\n自己写个KMeans的乐趣\n自己写的一个好处是你可以记录一下迭代过程中的中心点位置，然后拿来画图，对于KMeans的过程会有直观的感受，你也会发现收敛得很快。这就是我在2011年的时候用R写的代码出的图：\n\n此处还有一段用Python写的KMeans可供参考。\nKMeans还能干点啥？\n它能做图片压缩，对颜色进行聚类，然后用中心点去替换相似的颜色，比如说一张图，你给压缩成16色的，图片就可以小很多。这也是当年写了KMeans之后，干过的一件事情。然后还可以有一个类似的事情可以干，我把图片颜色的中心点拿出来，这不等到把配色方案拿到手了么？然后你就可以将其应用于自己的图中。也就是说你上班摸鱼在看图，就有借口说自己在找配色方案了。这就是当年写下这篇《食色性也》所介绍的。\n\nKMeans不能干啥\n从前面说到的KMeans的两个假设，就可以看出来，不符合这两个假设，它就不能干啊。因为KMeans只能确定线性聚类边界，所以当边界很复杂的时候，它是做不好的。比如我在《AI学起来，毕竟AI让地球运转》里放的这张用SVM分类的图，用KMeans就做不好。\n\n但是我们想一想，SVM是怎么做的？应用一个核函数，将数据投影到更高维的空间，然后是线性可分的，在高维空间进行分类。既然在高维空间是线性可分的，那么用KMeans也是可以的。所以像这种KMeans干不来的分类，如果我们组合一个核变换+KMeans，那么它也是干得来的。\n欲知后事如何，请听下回分解。"}}